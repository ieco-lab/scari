---
title: "Extract monthly mean temperatures to create DD suitability maps"
author: "Sam Owens"
contact: "sam.owens@temple.edu"
date: "2023-08-31"
output: html_document
---

# Overview

This vignette is a dependency of vignette 04, "04_preliminary_MaxEnt_models_setup.Rmd".

This vignette will extract climate rasters from [CHELSA](https://chelsa-climate.org/timeseries/) to data frames. These data are historical temperature means from 1981-2010. They are further divided by month. Each file is a global-scale raster containing the average temperature for that month and year. These will be extracted to a final data frame with with one column per month and the averaged means over the 30 year period. 

These data will be used to produce a binary suitability map via the Lewkiewicz et.al PDE model for SLF. This model can be called using a built-in function called `slfSpread::compute_DD_suitability()`, which will be used in vignette 04. The final data produced from this model will also be manipulated and explored in vignette 04. 

### Versioning

The code for this vignette was originally taken from "slfSpread_wc2.1_historical_temps_extract.Rmd", which is stored in `slfSpread/sandbox`.

# Setup

```{r library necessary packages}

library(tidyverse)
library(devtools)

library(here)
here() # here() starts at the root folder of this package

library(terra)
library(geodata) # worldclim data retrieval

```

# Download rasters to directory

First, I will download the CHELSA data using the direct links that were taken from the CHELSA data portal. These are stored in the `data-raw` folder within the root folder of this package.

```{r download monthly mean CHELSA data}

if(FALSE) {

  chelsa_monthly_URLs <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_1981-2010_monthly_mean_temp_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")

  # loop to download URLs
  for(i in 1:nrow(chelsa_monthly_URLs)) {
    
    file.tmp <- chelsa_monthly_URLs[i, ]
    
    utils::browseURL(url = file.tmp)
    
  }

}

```

A few of the initial files were missing or got stuck in their temporary download form. It is worth joining the list of file names with the list of files that were actually downloaded to a directory, which is what I did in this next chunk.

```{r ensure all files downloaded properly}

# file path to local directory
mypath <- file.path(here() %>% 
                     dirname(),
                   "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# URLs list

# convert to df
as.data.frame(chelsa_monthly_URLs)
# isolate file names from original text file
chelsa_monthly_URLs$URL <- chelsa_monthly_URLs$URL %>%
  gsub(pattern = "https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/monthly/tas/", replacement = "") 
# rename column for join
colnames(chelsa_monthly_URLs) <- "file"

# downloads list

# get list of files downloaded to directory
chelsa_monthly_downloads <- as_tibble(list.files(file.path(mypath, "originals", "tavg"))) %>%
  rename("file" = "value")

# join

# anti join the two tibbles to check for missing files
anti_join(x = chelsa_monthly_URLs, y = chelsa_monthly_downloads)

# remove wrong files
file.remove(file.path(mypath, "originals/tavg/CHELSA_tas_07_2007_V.2.1.tif.crdownload"))
file.remove(file.path(mypath, "originals/tavg/CHELSA_tas_07_1993_V.2.1.tif.crdownload"))

```

```{r download files that failed as necessary}

chelsa_monthly_URLs2 <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_1981-2010_monthly_mean_temp_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")

# select URLs that failed
chelsa_monthly_URLs2 <- slice(.data = chelsa_monthly_URLs2, c(193, 207, 213))

# loop to download historical URLs
for(i in 1:nrow(chelsa_monthly_URLs2)) {
  
  file.tmp <- chelsa_monthly_URLs2[i, ]
  
  utils::browseURL(url = file.tmp)
  
}

```

# Group, stack and average rasters per month

Currently, the directory of files contains one file per month and year of averaged temperature data. This equates to 30 global-scale rasters, representing the average temperature in that month from 1981-2010. I will need to subdivide the directory of files by month, stack the 30 rasters per month and average them. This will produce one averaged raster per month that I can then convert to a data frame.

```{r setup for loop}

# file path to local directory
mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# directory of files
chelsa_monthly_downloads2 <- as.data.frame(list.files(file.path(mypath, "originals", "tavg"))) 
# rename values
colnames(chelsa_monthly_downloads2) <- "file"

# create a 12 column data frame of files, which each column containing all year files for that month.
files_by_month <- data.frame(
  `01` = chelsa_monthly_downloads2[1:30, ],
  `02` = chelsa_monthly_downloads2[31:60, ],
  `03` = chelsa_monthly_downloads2[61:90, ],
  `04` = chelsa_monthly_downloads2[91:120, ],
  `05` = chelsa_monthly_downloads2[121:150, ],
  `06` = chelsa_monthly_downloads2[151:180, ],
  `07` = chelsa_monthly_downloads2[181:210, ],
  `08` = chelsa_monthly_downloads2[211:240, ],
  `09` = chelsa_monthly_downloads2[241:270, ],
  `10` = chelsa_monthly_downloads2[271:300, ],
  `11` = chelsa_monthly_downloads2[301:330, ],
  `12` = chelsa_monthly_downloads2[331:360, ]
)

```


```{r loop to average rasters by month}
  
if (FALSE){
  
    # loop to load in raster and stack it with the others from its month
    for (j in 2:ncol(files_by_month)){
      
      # setup
      
      # load in the each column as a list of files
      file.list <- list(file.path(mypath, "originals", "tavg", files_by_month[, j])) %>%
        unlist()
      # print first 10 in console
      print(head(file.list))
      
      # file name object
      file.name <- colnames(files_by_month[j]) %>%
        gsub(pattern = "X", replacement = "")
      
      # loop
      
      # covert list of file names to raster objects (NOT SURE if this step is necessary- might just be solved with sprc())
      #stack.hold <- terra::rast(file.list)
      # stack rasters into one using sprc
      stack.hold <- terra::sprc(x = file.list)
      # take average of 30 rasters
      stack.avg <- terra::mosaic(x = stack.hold, fun = "mean")
      # write raster stack to file
      writeRaster(x = stack.avg, 
                  filename = file.path(mypath, "v1_averaged", paste0("ch2.1_tavg_",  file.name, "_1981-2010.tif")), 
                  filetype = "GTiff", 
                  overwrite = FALSE
                  )
      
      # remove temp files
      rm(file.list)
      rm(file.name)
      rm(stack.hold)
      rm(stack.avg)
      
    }
  
}
      
      
```

This next chunk is to check values of the original vs the averaged rasters, because the initial run of this script gave a strange output. 

```{r check raster values}

mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

jan_tavg_1981 <- terra::rast(file.path(mypath, "originals", "tavg", files_by_month[1, 1]))

jan_tavg_averaged <- terra::rast(file.path(mypath, "v1_averaged", "ch2.1_tavg_01_1981-2010.tif"))

# minmax
terra::minmax(jan_tavg_1981)
terra::minmax(jan_tavg_averaged)

# gloabl mean
terra::global(jan_tavg_1981, fun = "mean")
terra::global(jan_tavg_averaged, fun = "mean")

```

I have found a number of issues with the data that have been averaged in the rasters. First, I realized after running this chunk that it is taking so long (so, so long) because CHELSA distributes its air temperature for the entire planet, including over the oceans. The layers will need to be cropped and masked using `global_atc`, as was done in vignette 03. Second, the raster resolution needs to be downscaled to the 5 arcminute scale. Third, the technical specifications for this type of CHELSA data indicate that these temperature data are in degrees Kelvin/10, whereas I need the data to be in degrees Celsius. I will correct these issues before proceeding.

# Mask and resample rasters to lower res

The `compute_DD_suitability()` in this package was designed to compute DD suitability for rasters at a 5 arcminute scale, because the 30 arcsecond would likely be too fine to compute data for and the fuction would take a very long time. CHELSA does not distribute its data at a coarser resolution, so I will need to aggregate these layers to 5 arcminutes using a worldclim raster. The 5 arcminute version of these data can be found [here](https://www.worldclim.org/data/worldclim21.html). I will provide code to download these data using the [geodata](https://rdrr.io/cran/geodata/) package in R, but I will be loading them in from file.

```{r download 5 arcmin raster using geodata package}


geodata::worldclim_global(var = "tavg",
                          res = 5,
                          path = "SET_PATH",
                          )

```

```{r load in reference layers}

# set path to external directory
mypath <- file.path(here() %>% 
                          dirname(),
                        "maxent/historical_climate_rasters/chelsa2.1_30arcsec/originals")

# global access to cities
global_atc <- terra::rast(x = file.path(mypath, "2015_accessibility_to_cities_v1.0.tif"))

# CHELSA bioclim layer as reference
global_bio1 <- terra::rast(x = file.path(mypath, "CHELSA_bio1_1981-2010_V.2.1.tif"))



# set path to external directory containing wc file
mypath <- file.path(here() %>% 
                          dirname(),
                        "maxent/historical_climate_rasters/wc2.1_5arcmin/originals/tavg")

# this layer is the 5 arcminute worldclim raster
wc_tavg_5arcmin <- terra::rast(x = file.path(mypath, "wc2.1_5m_tavg_01.tif"))

```

```{r harmonize reference layers}

# ext of the reference layers
ext(global_atc)
ext(global_bio1)
ext(wc_tavg_5arcmin)
# the extents will need to be corrected

# crs of reference layers
terra::crs(x = global_atc, proj = TRUE)
terra::crs(x = global_bio1, proj = TRUE)
terra::crs(x = wc_tavg_5arcmin, proj = TRUE)
# the crs is fine on both (ID = EPSG 4326), but will set anyhow

# origin
terra::origin(global_atc)
terra::origin(global_bio1)
terra::origin(wc_tavg_5arcmin)
# the origin of global_atc and wc_tavg_5arcmin will need to be changed to match the others

# edits to reference layers

# set extent
# set ext to the smallest whole number shared between the layers
ext.obj <- terra::ext(-180, 179, -60, 83)
# create a mask layer specifically for the cropping and masking
mask_layer <- terra::crop(x = global_atc, y = ext.obj)
# create layer for resampling
resample_layer <- terra::crop(x = wc_tavg_5arcmin, y = ext.obj)

# unify origins
# set origin of global_atc
terra::origin(mask_layer) <- c(-0.0001396088, -0.0001392488)
# set origin of worldclim 5 arcmin layer
terra::origin(resample_layer) <- c(-0.0001396088, -0.0001392488)


# resample
mask_layer <- terra::resample(x = mask_layer, y = main_layer, method = "bilinear")

# set CRS of reference layers to be sure
mask_layer <- terra::project(x = mask_layer, y = "EPSG:4326", method = "bilinear")
resample_layer <- terra::project(x = resample_layer, y = "EPSG:4326", method = "bilinear")

```

```{r get file names}
 
# file path to local directory
mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# I will load in the files and then get the new names I would like to give them

# load in tavg rasters to be cropped- the original .tif files
env.files <- list.files(path = file.path(mypath, "v1_averaged"), pattern = "\\.tif$", full.names = T)

# output file names
output.files <- list.files(path = file.path(mypath, "v1_averaged"), pattern = "\\.tif$", full.names = F) 

```

```{r check 1st raster specs}
  
# load in 1st averaged raster to check crs, origin and extent
test_layer <- terra::rast(x = env.files[1])

# extents
ext(test_layer)
ext(mask_layer)
# the extents will need to be corrected

# crs 
terra::crs(test_layer)
terra::crs(x = mask_layer)
# the crs is fine on all (ID = EPSG 4326)

# origins
terra::origin(test_layer)
terra::origin(mask_layer)
# the origins are fine on all

```

```{r loop to mask rasters and resample}

if (FALSE) {

  # file path to local directory
  mypath <- file.path(here() %>% 
                     dirname(),
                   "maxent/historical_climate_rasters/chelsa2.1_suitable_area")
  
    # loop to crop extent for all files
    for(a in seq_along(env.files)) {
      
      # load each raster into temp object
      rast.hold <- terra::rast(env.files[a])
      
      
      # begin edits
      # crop new rasters to extent
      rast.hold <- terra::crop(x = rast.hold, y = ext.obj)
      
      # mask the tavg layer by global_atc
      rast.hold <- terra::mask(x = rast.hold, mask = mask_layer)
      
      # resample to 5 arcmin spatial scale
      rast.hold <- terra::resample(x = rast.hold, y = resample_layer, method = "bilinear")
      
      #write out the new resampled rasters!
      terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1_masked_aggregated", output.files[a]), filetype = "GTiff", overwrite = FALSE)
      
      # remove object once its done
      rm(rast.hold)

  }
  
}

```

# convert units to Celsius

Finally, I will convert the units of these rasters from Kelvin/10 to Celsius. The `terra::app()` function will apply the function `convert_to_celsius()` within this package to each cell in the rasters. These new values are output as a data frame. This process will take quite a long time.

```{r get file names}
 
# file path to local directory
mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# I will load in the files and then get the new names I would like to give them

# load in tavg rasters to be cropped- the original .tif files
env.files <- list.files(path = file.path(mypath, "v1_masked_aggregated"), pattern = "\\.tif$", full.names = T)

# output file names
output.files <- list.files(path = file.path(mypath, "v1_masked_aggregated"), pattern = "\\.tif$", full.names = F) 

#file.names <- list.files(path = file.path(mypath, "v1_masked_aggregated"), pattern = "\\.tif$", full.names = F) %>%
#  gsub(pattern = "ch2.1_tavg_", replacement = "") %>%
#  gsub(pattern = "_1981-2010.tif", replacement = "") %>%

```

```{r apply unit conversion function}

if (FALSE) {

# loop to crop extent for all files
  for(a in seq_along(env.files)) {
    
    # load each raster into temp object
    rast.hold <- terra::rast(env.files[a])
    
    # apply Celsius conversion function to all cells
    rast.hold <- terra::app(x = rast.hold, fun = slfSpread::convert_to_celsius)
    
    #write raster
    terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1_final", output.files[a]), filetype = "GTiff", overwrite = FALSE)
      
    # remove object once its done
    rm(rast.hold)
    
  }
    
}

```

```{r test to see if conversion worked}

mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# read in rasters to compare
raster1 <- terra::rast(x = file.path(mypath, "v1_masked_aggregated", "ch2.1_tavg_01_1981-2010.tif"))
raster1_units_converted <- terra::rast(x = file.path(mypath, "v1_final", "ch2.1_tavg_01_1981-2010.tif"))

# get minmax values
terra::minmax(raster1)
terra::minmax(raster1_units_converted)

# apply function manually

# convert to df
raster1_df <- terra::as.data.frame(raster1) %>%
  head(10)
raster1_units_converted_df <- terra::as.data.frame(raster1_units_converted) %>%
  head(10)

raster1_df <- as.data.frame(apply(X = raster1_df, MARGIN = 1, FUN = slfSpread::convert_to_celsius))


```

It seems that the unit conversion worked.

# convert rasters to dfs

```{r get file names}
  
# file path to local directory
mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")


# list of .tif file names for conversion
env.files <- list.files(path = file.path(mypath, "v1_final"), pattern = "\\.tif$", full.names = T)
# list of df names for output from the loop
output.files <- list.files(path = file.path(mypath, "v1_final"), pattern = "\\.tif$", full.names = F) %>%
  gsub(pattern = ".tif", replacement = ".csv")

# list of names for column naming conversion
file.names <- list.files(path = file.path(mypath, "v1_final"), pattern = "\\.tif$", full.names = F) %>%
  gsub(pattern = "ch2.1_tavg_", replacement = "") %>%
  gsub(pattern = "_1981-2010.tif", replacement = "") 

```


```{r loop to convert rasters to .csv}

for (a in seq_along(env.files)) {
  
  # read the data
  data_import <- rast(env.files[a])
  
  # change cell value naming 
  names(data_import) <- paste0("ch2.1_tavg_", file.names[a], "_1981-2010")
  
  # convert to data.frame
  data_import <- terra::as.data.frame(data_import, cells = TRUE, xy = TRUE)
  
  # write as csv
  write_csv(x = data_import, file = file.path(mypath, "v1_final_dfs", output.files[a]))
  
}


```

# file consolidation

```{r get file names}

# file path to local directory
mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")


# list of .tif file names for conversion
env.files <- list.files(path = file.path(mypath, "v1_final_dfs"), full.names = T)


```

```{r tavg csv joining}

# the initial object that others will be joined to (the january df)- RUN THIS LINE BEFORE THE FOR LOOP
data_join <- read_csv(env.files[1])

# loop- RUN ABOVE CODE FIRST
for (a in 2:length(env.files)) {
  
  # read the other csvs into this object
  data_join <- read_csv(env.files[a]) %>%
    # join each other csv to the january csv. I am like 99% sure the lat, long and cell number are the same between months
    right_join(data_join, ., by = c("cell", "x", "y"))
  
}

# write as csv
write_csv(x = data_join, file = file.path(mypath, "ANNUAL_ch2.1_5m_tavg.csv"))
# I will also write it to my root folder
write_csv(x = data_join, file = file.path(here(), "vignette-outputs", "data-tables", "ANNUAL_ch2.1_5m_tavg.csv"))
# also save as an Rdata file
save(data_join, file = file.path(here(), "data", "ANNUAL_ch2.1_5m_tavg.rda"))
  
```

The above datasets were used to create a binary suitability based on DD accumulation. This function used a threshold number of degree days that is necessary for 50% of an SLF population to reach egg-laying stage. From this count, a binary suitability metric was computed. I need to convert this output from the DD function, which was calculated using the datasets above, into a raster and visualize it.

# References

Fick, S.E. and R.J. Hijmans, 2017. WorldClim 2: new 1km spatial resolution climate surfaces for global land areas. International Journal of Climatology 37 (12): 4302-4315.

Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, P., Kessler, M. (2017). Climatologies at high resolution for the Earth land surface areas. Scientific Data. 4 170122. https://doi.org/10.1038/sdata.2017.122

Karger D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E, Linder, H.P., Kessler, M. (2018): Data from: Climatologies at high resolution for the earth’s land surface areas. EnviDat. https://doi.org/10.16904/envidat.228.v2.1
