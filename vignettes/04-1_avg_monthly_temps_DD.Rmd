---
title: "Extract monthly mean temperatures to create DD suitability maps"
author: "Sam Owens"
contact: "sam.owens@temple.edu"
date: "2023-08-31"
output: html_document
---

# Overview

This vignette will extract climate rasters from [CHELSA](https://chelsa-climate.org/timeseries/) to data frames. These data are historical temperature means from 1981-2010. They are further divided by month. Each file is a global-scale raster containing tmax for that month and year. These will be extracted to a final data frame with with one column per month and the averaged means over the 30 year period. 

These data will be used to produce a binary suitability map via the Lewkiewicz et al PDE model for SLF. The final data produced from this model will also be manipulated and explored here.

### Versioning

The code for this vignette was originally taken from "slfSpread_wc2.1_historical_temps_extract.Rmd", which is stored in `slfSpread/sandbox`.

# Setup

```{r library necessary packages}

library(tidyverse)
library(devtools)

library(here)
here() # here() starts at the root folder of this package

library(terra)
library(geodata) # worldclim data retrieval

```

# Download rasters to directory

```{r download monthly mean CHELSA data}

if(FALSE) {

  chelsa_monthly_URLs <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_1981-2010_monthly_mean_temp_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")

  # loop to download URLs
  for(i in 1:nrow(chelsa_monthly_URLs)) {
    
    file.tmp <- chelsa_monthly_URLs[i, ]
    
    utils::browseURL(url = file.tmp)
    
  }

}

```

```{r ensure all files downloaded properly}

# file path to local directory
mypath <- file.path(here() %>% 
                     dirname(),
                   "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# URLs list

# convert to df
as.data.frame(chelsa_monthly_URLs)
# isolate file names from original text file
chelsa_monthly_URLs$URL <- chelsa_monthly_URLs$URL %>%
  gsub(pattern = "https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/monthly/tas/", replacement = "") 
# rename column for join
colnames(chelsa_monthly_URLs) <- "file"

# downloads list

# get list of files downloaded to directory
chelsa_monthly_downloads <- as_tibble(list.files(file.path(mypath, "originals", "tavg"))) %>%
  rename("file" = "value")

# join

# anti join the two tibbles to check for missing files
anti_join(x = chelsa_monthly_URLs, y = chelsa_monthly_downloads)

# remove wrong files
file.remove(file.path(mypath, "originals/tavg/CHELSA_tas_07_2007_V.2.1.tif.crdownload"))
file.remove(file.path(mypath, "originals/tavg/CHELSA_tas_07_1993_V.2.1.tif.crdownload"))

```

```{r download files that failed as necessary}

chelsa_monthly_URLs2 <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_1981-2010_monthly_mean_temp_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")

# select URLs that failed
chelsa_monthly_URLs2 <- slice(.data = chelsa_monthly_URLs2, c(193, 207, 213))

# loop to download historical URLs
for(i in 1:nrow(chelsa_monthly_URLs2)) {
  
  file.tmp <- chelsa_monthly_URLs2[i, ]
  
  utils::browseURL(url = file.tmp)
  
}

```

# Group, stack and average rasters per month

Currently, the directory of files contains one file per month and year of averaged temperature data. This equates to 30 global-scale rasters, representing the average temperature in that month from 1981-2010. I will need to subdivide the directory of files by month, stack the 30 rasters per month and average them. This will produce one averaged raster per month that I can then convert to a data frame.

```{r setup for loop}

# file path to local directory
mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# directory of files
chelsa_monthly_downloads2 <- as.data.frame(list.files(file.path(mypath, "originals", "tavg"))) 
# rename values
colnames(chelsa_monthly_downloads2) <- "file"

# create a 12 column data frame of files, which each column containing all year files for that month.
files_by_month <- data.frame(
  `01` = chelsa_monthly_downloads2[1:30, ],
  `02` = chelsa_monthly_downloads2[31:60, ],
  `03` = chelsa_monthly_downloads2[61:90, ],
  `04` = chelsa_monthly_downloads2[91:120, ],
  `05` = chelsa_monthly_downloads2[121:150, ],
  `06` = chelsa_monthly_downloads2[151:180, ],
  `07` = chelsa_monthly_downloads2[181:210, ],
  `08` = chelsa_monthly_downloads2[211:240, ],
  `09` = chelsa_monthly_downloads2[241:270, ],
  `10` = chelsa_monthly_downloads2[271:300, ],
  `11` = chelsa_monthly_downloads2[301:330, ],
  `12` = chelsa_monthly_downloads2[331:360, ]
)

```

```{r loop to average rasters by month}
  
if (FALSE){
  
    # loop to load in raster and stack it with the others from its month
    for (j in 2:ncol(files_by_month)){
      
      # setup
      
      # load in the each column as a list of files
      file.list <- list(file.path(mypath, "originals", "tavg", files_by_month[, j])) %>%
        unlist()
      # print first 10 in console
      print(head(file.list))
      
      # file name object
      file.name <- colnames(files_by_month[j]) %>%
        gsub(pattern = "X", replacement = "")
      
      # loop
      
      # covert list of file names to raster objects (NOT SURE if this step is necessary- might just be solved with sprc())
      #stack.hold <- terra::rast(file.list)
      # stack rasters into one using sprc
      stack.hold <- terra::sprc(x = file.list)
      # take average of 30 rasters
      stack.avg <- terra::mosaic(x = stack.hold, fun = "mean")
      # write raster stack to file
      writeRaster(x = stack.avg, 
                  filename = file.path(mypath, "v1_averaged", paste0("ch2.1_tavg_",  file.name, "_1981-2010.tif")), 
                  filetype = "GTiff", 
                  overwrite = FALSE
                  )
      
      # remove temp files
      rm(file.list)
      rm(file.name)
      rm(stack.hold)
      rm(stack.avg)
      
    }
  
}
      
      
```

I have found a number of issues with the data that have been averaged in the rasters. First, I realized after running this chunk that it is taking so long (so, so long) because CHELSA distributes its air temperature for the entire planet, including over the oceans. The layers will need to be cropped and masked using `global_atc`, as was done in vignette 03. Second, the technical specifications for this type of CHELSA data indicate that these temperature data are in degrees Kelvin/10, whereas I need the data to be in degrees Celsius. I will correct these issues before proceeding. 

# Mask new rasters

```{r load in reference layers}

# set path to external directory
mypath <- file.path(here() %>% 
                          dirname(),
                        "maxent/historical_climate_rasters/chelsa2.1_30arcsec/originals")

# global access to cities
global_atc <- terra::rast(x = file.path(mypath, "2015_accessibility_to_cities_v1.0.tif"))

# CHELSA bioclim layer as reference
global_bio1 <- terra::rast(x = file.path(mypath, "CHELSA_bio1_1981-2010_V.2.1.tif"))

```

```{r harmonize reference layers}

# ext of the reference layers
ext(global_atc)
ext(global_bio1)
# the extents will need to be corrected

# crs of reference layers
terra::crs(x = global_atc, proj = TRUE)
terra::crs(x = global_bio1, proj = TRUE)
# the crs is fine on both (ID = EPSG 4326), but will set anyhow

# origin
terra::origin(global_atc)
terra::origin(global_bio1)
# the origin of global_atc will need to be changed to match the others

# edits to reference layers

# set extent
# set ext to the smallest whole number shared between the layers
ext.obj <- terra::ext(-180, 179, -60, 83)
# create main layer for future cropping, crop to new ext
main_layer <- terra::crop(x = global_bio1, y = ext.obj, overwrite = FALSE)
# create a mask layer specifically for the cropping and masking
mask_layer <- terra::crop(x = global_atc, y = ext.obj, overwrite = FALSE)

# unify origins
# set origin of global_atc
terra::origin(mask_layer) <- c(-0.0001396088, -0.0001392488)
# resample
mask_layer <- terra::resample(x = mask_layer, y = main_layer, method = "bilinear")

# set CRS of reference layers to be sure
main_layer <- terra::project(x = main_layer, y = "EPSG:4326", method = "bilinear")
mask_layer <- terra::project(x = mask_layer, y = "EPSG:4326", method = "bilinear")

```

```{r get file names}
 
# file path to local directory
mypath <- file.path(here() %>% 
                   dirname(),
                 "maxent/historical_climate_rasters/chelsa2.1_suitable_area")

# I will load in the files and then get the new names I would like to give them

# load in tavg rasters to be cropped- the original .tif files
env.files <- list.files(path = file.path(mypath, "v1_averaged"), pattern = "\\.tif$", full.names = T)

# output file names
output.files <- list.files(path = file.path(mypath, "v1_averaged"), pattern = "\\.tif$", full.names = F) 

```

```{r loop to tidy rasters}
  
# load in 1st averaged raster to check crs, origin and extent
test_layer <- terra::rast(x = env.files[1])

# extents
ext(test_layer)
ext(mask_layer)
ext(main_layer)
# the extents will need to be corrected

# crs 
terra::crs(test_layer)
terra::crs(x = mask_layer)
terra::crs(x = main_layer)
# the crs is fine on all (ID = EPSG 4326)

# origins
terra::origin(test_layer)
terra::origin(mask_layer)
terra::origin(main_layer)
# the origins are fine on all

ext.obj <- terra::ext(-180, 179, -60, 83)

if (TRUE) {

  # file path to local directory
  mypath <- file.path(here() %>% 
                     dirname(),
                   "maxent/historical_climate_rasters/chelsa2.1_suitable_area")
  
    # loop to crop extent for all files
    for(a in seq_along(env.files)) {
      
      # load each raster into temp object
      rast.hold <- terra::rast(env.files[a])
      
      
      # begin edits
      # crop new rasters to extent
      rast.hold <- terra::crop(x = rast.hold, y = ext.obj, overwrite = FALSE)
      
      # mask the tavg layer by global_atc
      rast.hold <- terra::mask(x = rast.hold, mask = mask_layer)
      
      #write out the new resampled rasters!
      terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1_masked", output.files[a]), filetype = "GTiff", overwrite = FALSE)
      
      # remove object once its done
      rm(rast.hold)

  }
  
}

```

# convert raster units to Celsius

```{r convert units of rasters}



```

# Aggregate rasters at 5 arcminute scale

The `compute_DD_suitability()` in this package was designed to compute DD suitability for rasters at a 5 arcminute scale, because the 30 arcsecond would likely be too fine to compute data for and the fuction would take a very long time. CHELSA does not distribute its data at a coarser resolution, so I will need to aggregate these layers to 5 arcminutes using a worldclim raster. The 5 arcminute version of these data can be found [here](https://www.worldclim.org/data/worldclim21.html). I will provide code to download these data using the [geodata](https://rdrr.io/cran/geodata/) package in R, but I will be loading them in from file.

```{r download data using geodata package}


geodata::worldclim_global(var = "tavg",
                          res = 5,
                          path = "SET_PATH",
                          )

```


```{r load in reference layer}

# set path to external directory
mypath <- file.path(here() %>% 
                          dirname(),
                        "maxent/historical_climate_rasters/wc2.1_5arcmin/originals/tavg")

wc_tavg_5arcmin <- terra:rast(x = file.path(mypath, "wc2.1_5m_tavg_01.tif"))

```

```{r prepare reference layer}

# check that extents are same

```


```{r aggregate}

rast.hold <- terra::resample(x = rast.hold, y = main_layer, method = "bilinear")

```







# convert rasters to dfs

```{r tavg to df loop}

if (FALSE) {

  # ONLY EDIT THESE TWO OBJECTS
  
  # file path to local directory
  mypath <- file.path(here() %>% 
                     dirname(),
                   "maxent/historical_climate_rasters/chelsa2.1_suitable_area")
  
  data_type <- "tavg"
  
  
  
  
  
  # First, I need to create a directory of file paths and of final file names
  
  # pathing objects
  
  # list of input file paths
  input_paths <- list.files(file.path(mypath, "originals", data_type), full.names = TRUE)
  
  input_paths[1]
  
  # file path for output
  output_path <- gsub(pattern = paste0("originals/", data_type, "/wc2.1_5m_", data_type, "_01.tif"), replacement = paste0("dfs/", data_type), x = input_paths[1])
  
  # naming objects
  
  # list of .tif file names for conversion
  input_names <- list.files(file.path(mypath, "originals", data_type), full.names = FALSE)
  # list of df names for output from the loop
  output_names <- gsub(pattern = ".tif", replacement = ".csv", x = input_names)
  
  
  # check
  
  # check input paths
  input_paths[5]
  # check output path
  paste0(output_path, output_names[5])
  
  
  
  # loop
  for (i in seq_along(1:length(input_paths))) {
    
    # read the data
    data_import <- rast(input_paths[i])
    
    # convert to data.frame
    data_import <- terra::as.data.frame(data_import, cells = TRUE, xy = TRUE)
    
    # write as csv
    write_csv(x = data_import, file = file.path(output_path, output_names[i]))
    
  }
  
}

```

# file consolidation

```{r tavg csv joining}

# ONLY EDIT THESE TWO OBJECTS

# my path
mypath <- file.path(here() %>%
                   dirname(),
                 "maxent", "historical_climate_rasters", "wc2.1_5arcmin")

data_type <- "tavg"

# First, I need to create a directory of file paths and of final file names

# pathing objects

# list of input file paths
input_paths <- list.files(file.path(mypath, "dfs", data_type), full.names = TRUE)

# file path for output
output_path <- file.path(mypath, "dfs")


# the initial object that others will be joined to (the january df)- RUN THIS LINE BEFORE THE FOR LOOP
data_join <- read_csv(input_paths[1])

# loop- RUN ABOVE CODE FIRST
for (i in 2:length(input_paths)) {
  
  # read the other csvs into this object
  data_join <- read_csv(input_paths[i]) %>%
    # join each other csv to the january csv. I am like 99% sure the lat, long and cell number are the same between months
    right_join(data_join, ., by = c("cell", "x", "y"))
  
}

# write as csv
write_csv(x = data_join, file = file.path(output_path, paste0("ANNUAL_wc2.1_5m_", data_type, ".csv")))
  
```

The above datasets were used to create a binary suitability based on DD accumulation. This function used a threshold number of degree days that is necessary for 50% of an SLF population to reach egg-laying stage. From this count, a binary suitability metric was computed. I need to convert this output from the DD function, which was calculated using the datasets above, into a raster and visualize it.

# References

Fick, S.E. and R.J. Hijmans, 2017. WorldClim 2: new 1km spatial resolution climate surfaces for global land areas. International Journal of Climatology 37 (12): 4302-4315.

Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, P., Kessler, M. (2017). Climatologies at high resolution for the Earth land surface areas. Scientific Data. 4 170122. https://doi.org/10.1038/sdata.2017.122

Karger D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E, Linder, H.P., Kessler, M. (2018): Data from: Climatologies at high resolution for the earth’s land surface areas. EnviDat. https://doi.org/10.16904/envidat.228.v2.1
