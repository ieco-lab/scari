---
title: "Retrieve bioclim rasters from CHELSA and tidy them for MaxEnt modeling"
author: "Sam Owens"
date: "2023-07-17"
contact: 'sam.owens@temple.edu'
output: html_document
---

This vignette will walk through the process of downloading and tidying CHELSA climate rasters. [CHELSA](https://chelsa-climate.org/) represents a high-resolution (30 arc-seconds, ~1km) land surface dataset provided by the Swiss Federal Institute for Forest, Snow and Landscape Research. The specific data we are interested in is the suite of 19 "bioclimatic" variables. These [bioclim variables](https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf) are biologically relevant climate features derived from temperature and precipitation. CHELSA also makes the same variables available as climate change predictions, using a subset of the available [CMIP6](https://pcmdi.llnl.gov/CMIP6/) models. 

First, I will download the data from CHELSA into a local library using ...

Second, I will tidy the data and put them in the proper format for MaxEnt. 

Third, I will assess the level of co-linearity between the rasters

Fourth, I will visualize the rasters.

# Setup

```{r load necesssary packages, echo = FALSE}

library(tidyverse)  #data manipulation

library(here) #making directory pathways easier on different instances
here()
# here() is ".../Shared drives/slfClimate/projects/slfSpread/slfSpread_pkg"

library(devtools)
# devtools::install_github("danlwarren/ENMTools")
library(ENMTools) # env covariates colinearity

library(terra)
library(patchwork)

```

# 1. Retrieve CHELSA bioclim rasters

First, I will download the bioclim data into a local directory. I recommend downloading these data into a separate directory from this package. Since I am using a directory outside of the immediate package, I will create an object to define the file path at the beginning of each chunk. This pathing will still use `here::here()` to start at the package root folder. First, I will download the historical climate data from CHELSA. The URLs to the bioclim data that were downloaded from CHELSA are stored in `data-raw/CHELSA` within the package directory. I will be using two different versions of the CHELSA bioclim variables for my analysis: 

1. Historical- time period 1981-2010
2. Climate change predictions- time period 2041-2070, GFDL-ESM4 model, SSP3-70

The CHELSA download directory for each group of [bioclim variables](https://envicloud.wsl.ch/#/?prefix=chelsa%2Fchelsa_V2%2FGLOBAL%2F) also contains 27 other distinct climatologies, available for both historical datasets and climate change models. The URLs for these were also downloaded, as some of them might be applicable to this analysis. For now, we will download the 19 bioclim variables. We will also download and include four metrics of the Degree Day with a 10C threshold. If you would like to download any of the additional climatologies available from CHELSA, see code for this in the Appendix section of this vignette. Those chunks should be run after the initial run of all chunks before section 2.2. After running these chunks, continue from chunk "loop to tidy rasters" in section 2.2.

### 1.1 Historical

My first approach to downloading these data is to create a loop to open each URL in a web browser (be ready, as it literally just opens whatever URLs are selected from the list in a browser). Later, I will functionalize this so that the download can be more flexible. 

```{r download historical CHELSA data}

if(FALSE) {

  # historical data
  chelsa_historic_URLs <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_1981-2010_bioclim_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")
  
  # select only URLs I am interested in 
  chelsa_historic_URLs <- slice(.data = chelsa_historic_URLs, 2:20)
  
  # loop to download historical URLs
  for(i in 1:nrow(chelsa_historic_URLs)) {
    
    file.tmp <- chelsa_historic_URLs[i, ]
    
    utils::browseURL(url = file.tmp)
    
  }

}

```

With the current loop construction, these files can only be downloaded to my PC's downloads folder, so they will need to be manually moved to the destination directory (outside of the package root folder because they are large). These files were stored manually in `"maxent/historical_climate_rasters/chelsa2.1_30arcsec/originals"`.

### 1.2 CMIP6

Next, I will download data from the provided CMIP6 models. I will be downloading data the SSP3-70 [SSP scenarios](https://www.carbonbrief.org/explainer-how-shared-socioeconomic-pathways-explore-future-climate-change/) (see link for an explanation of these scenarios). I will use the [GFDL-ESM4](https://www.wdc-climate.de/ui/cmip6?input=CMIP6.CMIP.NOAA-GFDL.GFDL-ESM4) CMIP6 model. According to the CHELSA documentation, if all five available models are not used together, then CMIP6 model usage should follow the given priority (see documentation linked above).

```{r download CMIP6 CHELSA data}

if(FALSE) {

  # ssp370 data
  chelsa_370_URLs <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_2041-2070_GFDL-ESM4_ssp370_bioclim_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")
  
  
  # select only URLs I am interested in 
  chelsa_370_URLs <- slice(.data = chelsa_370_URLs, 1:19)
  
  # loop to download ssp370 URLs
  for(i in 1:nrow(chelsa_370_URLs)) {
    
    file.tmp <- chelsa_370_URLs[i, ]
    
    utils::browseURL(url = file.tmp)
    
  }

}

```

These files were stored manually in `"maxent/future_climate_rasters/chelsa2.1_30arcsec/originals"`.

# 2. Tidy for MaxEnt

Next, I need to tidy these rasters for MaxEnt. I will need to mask any data that is over the oceans or other bodies of water. I will also need to create downsized versions of these variables that can be used in step 4 to assess the level of correlation between the different variables. Lastly, I will rename the files and convert them to the required `.ascii` format for MaxEnt.

I will mask the CHELSA rasters using a dataset called ["global access to cities"](http://www.nature.com/articles/nature25181), which is a proxy for human impact and will be used in the final anaylsis. This dataset contains high-resolution mapping of the continents, so it is a good choice as a reference layer for the masking. I will use `bio1` (annual mean temperature) as a reference to resample the new layers after setting the CRS and extent.

## 2.1 Load in files

```{r load in reference layers}

# set path to external directory
mypath <- file.path(here() %>% 
                          dirname(),
                        "maxent/historical_climate_rasters/chelsa2.1_30arcsec/originals")

# global access to cities
global_atc <- terra::rast(x = file.path(mypath, "2015_accessibility_to_cities_v1.0.tif"))

# 1st bioclim layer as reference
global_bio1 <- terra::rast(x = file.path(mypath, "CHELSA_bio1_1981-2010_V.2.1.tif"))

```

We will get a list of the files that I need to modify. We will also create an object containing the new names assigned to the files to simplify the naming.

```{r get file names}
 
# file path to directory
mypath <- file.path(here() %>% 
                      dirname(),
                    "maxent/historical_climate_rasters/chelsa2.1_30arcsec")

# I will load in the files and then get the new names I would like to give them

# load in bioclim layers to be cropped- the original .tif files
env.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = T)

# output file names
output.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = F) %>%
  # get rid of filetype endings
  gsub(pattern = ".tif", replacement = "") %>%
  # crop off ending 
  gsub(pattern = "_V.2.1", replacement = "") %>%
  tolower()

# more edits to file names
output.files <- output.files %>%
  gsub(pattern = "chelsa_", replacement = "") %>%
  # specifically edit the access to cities naming
  gsub(pattern = "2015_accessibility_to_cities_v1.0", replacement = "atc_2015")

```

Now that we have our files and reference layers loaded in and names chosen, we can begin reformatting these layers.

## 2.2 Prepare reference layers

The first step is to crop the reference layers `global_atc` and `global_bio1` to the desired extent. We will use the smallest extent shared between both layers. We need to check on the correct CRS to use because the `proj4` string notation (which I had previously used) is expiring. The `proj4` string I had previously been using was `"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"`. 

In this chunk, we will create 2 reference layers. The `main_layer (global_bio1)` will be used for resampling the data, while the `mask_layer (global_atc)` will be used for masking.

```{r crop}

# ext of the reference layer
ext(global_atc)
# ext of the bioclim layers
ext(global_bio1)

# set ext to the smallest whole number shared between the layers
ext.obj <- terra::ext(-180, 179, -60, 83)

# create main layer for future cropping, crop to new ext
main_layer <- terra::crop(x = global_bio1, y = ext.obj, overwrite = FALSE)
# create a mask layer specifically for the cropping and masking
mask_layer <- terra::crop(x = global_atc, y = ext.obj, overwrite = FALSE)


# The Proj4string notation is expiring, so I will try to use an EPSG code equivalent
# Here is the proj4 string I was previously using:
# "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

# here is the current proj4 of global_atc
terra::crs(x = main_layer, proj = TRUE)

# they do not match. The terra documentation recommends using the EPSG code, so I will search for the correct one. 
# EPSG:4326 matches the current proj4 string and is in the correct formatting.
  
```

## 2.3 Tidying historical data

Here is where we get into tidying the data. We will crop the rasters using the extent object created from the reference layers. We will follow up the cropping with masking, which will convert cells in `x` that do not have a value in `global_atc` to `NAs`. I will also set the CRS to `EPSG:4326`, which is equivalent to the `proj4` string I had previously been using, and resample the rasters. The rasters will be resampled to the resolution of `global_bio1` so they have the same resolution. The output files will be saved in `v1`. 

This chunk will take a very long time to run... so maybe find a good book to read.

```{r loop to tidy rasters}
  
if(FALSE) {

  # file path to directory
  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/chelsa2.1_30arcsec")
  
  # view list of filetypes for terra, use .ascii
    terra::gdal(drivers = TRUE)
  
    # loop to crop extent for all files
    for(a in seq_along(env.files)) {
      
      # load each raster into temp object
      rast.hold <- terra::rast(env.files[a])
      
      
      
      # begin edits
      # crop new rasters to extent
      rast.hold <- terra::crop(x = rast.hold, y = ext.obj, overwrite = FALSE)
      
      # mask the bioclim layer by global_atc
      rast.hold <- terra::mask(x = rast.hold, mask = mask_layer)
      
      # set crs
      crs(rast.hold) <- "EPSG:4326"
      
      #resample to fit the extent/resolution of the main layer global_bio1
      #use bilinear interpolation, since values are continuous
      rast.hold <- terra::resample(x = rast.hold, y = main_layer, method = "bilinear")
      
      #write out the new resampled rasters!
      terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1", paste0(output.files[a], "_global", ".tif")), filetype = "GTiff", overwrite = FALSE)
      
      # remove object once its done
      rm(rast.hold)

  }
  
}

```

Finally, we will convert the rasters to the `.ascii` format required by MaxEnt. We will also convert all `NAs` to `-9999`, as required by MaxEnt.

```{r conversion of .tif to .asc files}

if(FALSE){
  
  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/chelsa2.1_30arcsec")
  
  # directory of files to modify
  env.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = TRUE) 
  # output file names
  output.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = FALSE) %>%
    gsub(pattern = ".tif", replacement = ".asc")
  
  # loop to convert to .ascii, set NAs equal to -9999 as required by Maxent
  for(a in seq_along(env.files)){
    
    # holding object  
    rast.hold <- terra::rast(env.files[a])
    
    # convert NAs to -9999
    rast.hold <- terra::subst(x = rast.hold, from = NA, to = -9999)
    # write output
    terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1_maxent", output.files[a]), filetype = "AAIGrid", overwrite = FALSE)
    
    # remove object once its done
    rm(rast.hold)
      
    }
    
}

```

These data can now be used in a MaxEnt model!

## 2.4 Tidying CMIP6 data

We will need to go through all of the above steps in section 2.3 for the CMIP6 versions of the rasters. 

```{r get file names}
 
# file path to directory
mypath <- file.path(here() %>% 
                      dirname(),
                    "maxent/future_climate_rasters/chelsa2.1_30arcsec/2041-2070_ssp370_GFDL")

# I will load in the files and then get the new names I would like to give them

# load in bioclim layers to be cropped- the original .tif files
env.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = TRUE)

# output file names
output.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = FALSE) %>%
  # get rid of endings
  gsub(pattern = "_V.2.1.tif", replacement = "") %>%
  # crop out some detail
  gsub(pattern = "-esm4_ssp", replacement = "_") %>%
  tolower()

# more edits to file names
output.files <- output.files %>%
  gsub(pattern = "chelsa_", replacement = "") 

```

```{r loop to tidy rasters}
  
if(FALSE) {

  # file path to directory
  mypath <- file.path(here() %>% 
                      dirname(),
                    "maxent/future_climate_rasters/chelsa2.1_30arcsec/2041-2070_ssp370_GFDL")
  
  # view list of filetypes for terra, use .ascii
    terra::gdal(drivers = TRUE)
  
    # loop to crop extent for all files
    for(a in seq_along(env.files)) {
      
      # load each raster into temp object
      rast.hold <- terra::rast(env.files[a])
      
      
      
      # begin edits
      # crop new rasters to extent
      rast.hold <- terra::crop(x = rast.hold, y = ext.obj, overwrite = FALSE)
      
      # mask the bioclim layer by global_atc
      rast.hold <- terra::mask(x = rast.hold, mask = mask_layer)
      
      # set crs
      crs(rast.hold) <- "EPSG:4326"
      
      #resample to fit the extent/resolution of the main layer global_bio1
      #use bilinear interpolation, since values are continuous
      rast.hold <- terra::resample(x = rast.hold, y = main_layer, method = "bilinear")
      
      #write out the new resampled rasters!
      terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1", paste0(output.files[a], "_global", ".tif")), filetype = "GTiff", overwrite = FALSE)
      
      # remove object once its done
      rm(rast.hold)

  }
  
}

```

```{r conversion of .tif to .asc files}

if(TRUE){
  
  mypath <- file.path(here() %>%
                        dirname(),
                      "maxent/future_climate_rasters/chelsa2.1_30arcsec/2041-2070_ssp370_GFDL")
  
  # directory of files to modify
  env.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = TRUE) 
  # output file names
  output.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = FALSE) %>%
    gsub(pattern = ".tif", replacement = ".asc")
  
  # loop to convert to .ascii, set NAs equal to -9999 as required by Maxent
  for(a in seq_along(env.files)) {
    
    # holding object  
    rast.hold <- terra::rast(env.files[a])
    
    # convert NAs to -9999
    rast.hold <- terra::subst(x = rast.hold, from = NA, to = -9999)
    # write output
    terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1_maxent", output.files[a]), filetype = "AAIGrid", overwrite = FALSE)
    
    # remove object once its done
    rm(rast.hold)
      
    }
    
}

```

# 3. Assess Colinearity

Before using any of these layers in a MaxEnt model, we need to ensure that none of the layers are significantly correlated to each other (I expect that most of them will be, as they are all derived from either temperature or precipitation). I will be using a threshold of 0.7 pearson's correlation or lower to select which layers I use.

We will downsize the rasters from the `v1` folder to perform a co-linearity analysis between the layers. This will only be done for the historical variables. We will be using `terra::aggregate()` to combine cells in a 2x2 grid fashion and take the mean of the 4 cells.

```{r downsize raster resolution}

if(FALSE) {
  
  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/chelsa2.1_30arcsec")
  
  #list env layers, load
  env.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = TRUE)
  output.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = FALSE)
  # I had to modify "pattern" to not include any .tif.aux.xml files
  
  # loop to downsample rasters for colinearity analysis
  for(a in seq_along(env.files)){
    
    # holding object
    rast.hold <- terra::rast(env.files[a])
    
    rast.hold <- terra::aggregate(rast.hold, 
                                  fact = 2, # downsampling by factor of 2 (read 2 cells deep around cell) and take mean of cells
                                  fun = mean, # take mean of these cells
                                  # expand = TRUE, # a carryover from raster::aggregate, terra does this automatically
                                  na.rm = TRUE, 
                                  filename = file.path(mypath, "v1_downsampled", output.files[a]), overwrite = FALSE)
    
    # remove object once its done
    rm(rast.hold)
    
  }
  
}

```

Now, we will perform the correlation using `ENMTools::raster.cor.matrix()`. This chunk will also take awhile, so its time to break out that book again.

```{r Take pearson correlation of raster layers}

if(TRUE){
  
  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/chelsa2.1_30arcsec")
  
  # load downsampled layers and stack for raster.cor.matrix command

  # list of layer paths
  env.files <- list.files(path = file.path(mypath, "v1_downsampled"), pattern = "\\.tif$", full.names = TRUE)
  
  # stack downsampled layers
  env <- c(x = terra::rast(env.files))
  # use nlyr, a command of the function terra::dimensions() to see if stacking was successful
  nlyr(env)
  
  
  # create a correlation matrix for picking model layers
  env.cor <- ENMTools::raster.cor.matrix(env = env, method = "pearson")
  
  # write out correlations as .csv.
  write.csv(x = env.cor, file = file.path(here(), "vignette-outputs", "data-tables", "env_cor_chelsa_downsampled.csv"), col.names = TRUE, row.names = TRUE)
  
}

```

We will use `ENMTools::raster.cor.plot()` from the same package to plot the results of the correlation analysis.

```{r plot correlation matrix}

ENMTools::raster.cor.plot(env)

```



```{r tidy env.cor}

env.cor2 <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "env_cor_chelsa_downsampled.csv")) 

env.cor2_tidy <- as.data.frame(env.cor2) 
# convert rownames column to rownames
row.names(env.cor2_tidy) <- env.cor2_tidy$...1
# get rid of ...1 column
env.cor2_tidy <- env.cor2_tidy[, which(names(env.cor2_tidy) != "...1")]

# reorder columns and take absolute values of correlations
env.cor2_tidy <- env.cor2_tidy %>%
  select(order(colnames(env.cor2_tidy))) %>%
  abs(.)

write.csv(x = env.cor2_tidy, file = file.path(here(), "data", "env_cor_chelsa_downsampled_abs.csv"), col.names = TRUE, row.names = TRUE)

# row names will be re-ordered in excel

```



# 4. Visualize rasters

```{r }

mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/chelsa2.1_30arcsec")

# load in bio 1
bio1 <- terra::rast(x = file.path(mypath, "v1_downsampled", "bio1_1981-2010_global.tif"))

bio1_plot <- ggplot() +
  geom_tile(data = as.data.frame(bio1), 
              aes(x = x, y = y, fill = `bio1_1981-2010_global`)) +
  xlab("longitude") +
  ylab("latitude") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "CHELSA bio 1") +
  coord_quickmap()

# load in gdd 10
gdd10 <- terra::rast(x = file.path(mypath, "v1_downsampled", "gdd10_1981-2010_global.tif"))

gdd10_plot <- ggplot() +
  geom_tile(data = as.data.frame(bio1), 
              aes(x = x, y = y, fill = `bio1_1981-2010_global`)) +
  xlab("longitude") +
  ylab("latitude") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "CHELSA 10C Growing Degree Days") +
  coord_quickmap()

# patchwork
bio1_plot / gdd10_plot

```



# Appendix

## Additional environmental covariates  

Along with the 19 traditional bioclimatic variables, CHELSA has created about [29 additional variables](https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf) that may help to explain the SLF invasion in N. America. I will include 4 metrics of growing degree day. Several studies have attempted to characterize the lower threshold for egg development and have found numbers between 8-13C (Maino et.al, 2022). Maino et.al also found that egg mortality increased significantly under 10C (Maino et.al, 2022). We will include various measures of degree day with a 10C threshold. Here are the additional variables I will add to the models:

* `GDD10`: Growing degree days heat sum above 10°C
* `NGD10`: Number of growing degree days
* `GDGFGD10`: First growing degree day above 10°C
* `GDDLGD10`: Last growing degree day above 10°C

### Download historical DD variables

```{r download historical CHELSA data}

if(FALSE) {

  # historical data
  chelsa_historic_URLs <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_1981-2010_bioclim_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")
  
  # select only DD URLs
  chelsa_historic_URLs <- slice(.data = chelsa_historic_URLs, c(32, 35, 38, 55))
  
  # loop to download historical URLs
  for(i in 1:nrow(chelsa_historic_URLs)) {
    
    file.tmp <- chelsa_historic_URLs[i, ]
    
    utils::browseURL(url = file.tmp)
    
  }

}

```

Files were placed in `"maxent/historical_climate_rasters/chelsa2.1_30arcsec/originals"`.

```{r get file names}
 
# file path to directory
mypath <- file.path(here() %>% 
                      dirname(),
                    "maxent/historical_climate_rasters/chelsa2.1_30arcsec")

# I will load in the files and then get the new names I would like to give them

# load in bioclim layers to be cropped- the original .tif files
env.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = T) %>%
  # extract the DD variables
    grep("gdd10|gddlgd10|gdgfgd10|ngd10", ., value = TRUE)

# output file names
output.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = F) %>%
  # extract the DD variables
  grep("gdd10|gddlgd10|gdgfgd10|ngd10", ., value = TRUE) %>%
  # get rid of filetype endings
  gsub(pattern = ".tif", replacement = "") %>%
  # crop off ending 
  gsub(pattern = "_V.2.1", replacement = "") %>%
  tolower()

# more edits to file names
output.files <- output.files %>%
  gsub(pattern = "chelsa_", replacement = "") 

```





### Download CMIP6 DD variables

```{r download CMIP6 CHELSA data}

if(FALSE) {

  # ssp370 data
  chelsa_370_URLs <- read_table(file = file.path(here(), "data-raw", "CHELSA", "chelsa_2041-2070_GFDL-ESM4_ssp370_bioclim_URLs.txt"),
                                col_names = FALSE) %>%
    as.data.frame() %>%
    dplyr::select("X1") %>%
    dplyr::rename("URL" = "X1")
  
  
  # select only DD URLs 
  chelsa_370_URLs <- slice(.data = chelsa_370_URLs, c(23, 26, 29, 42))
  
  # loop to download ssp370 URLs
  for(i in 1:nrow(chelsa_370_URLs)) {
    
    file.tmp <- chelsa_370_URLs[i, ]
    
    utils::browseURL(url = file.tmp)
    
  }

}

```

These files were stored manually in `"maxent/future_climate_rasters/chelsa2.1_30arcsec/originals"`.

```{r get file names}
 
# file path to directory
mypath <- file.path(here() %>%
                      dirname(),
                    "maxent/future_climate_rasters/chelsa2.1_30arcsec/2041-2070_ssp370_GFDL")

# I will load in the files and then get the new names I would like to give them

# load in bioclim layers to be cropped- the original .tif files
env.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = T) %>%
  # extract the DD variables
  grep("gdd10|gddlgd10|gdgfgd10|ngd10", ., value = TRUE)

# output file names
output.files <- list.files(path = file.path(mypath, "originals"), pattern = "\\.tif$", full.names = F) %>%
  # extract the DD variables
  grep("gdd10|gddlgd10|gdgfgd10|ngd10", ., value = TRUE) %>%
  # get rid of endings
  gsub(pattern = "_V.2.1.tif", replacement = "") %>%
  # crop out some detail
  gsub(pattern = "-esm4_ssp", replacement = "_") %>%
  tolower()

# more edits to file names
output.files <- output.files %>%
  gsub(pattern = "chelsa_", replacement = "") 

```







# References

Huron, N. A., Behm, J. E. & Helmus, M. R. 2022. Paninvasion severity assessment of a U.S. grape pest to disrupt the global wine market. Communications Biology, 5:1–11. https://doi.org/10.1038/s42003-022-03580-w

Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, P., Kessler, M. (2017). Climatologies at high resolution for the Earth land surface areas. Scientific Data. 4 170122. https://doi.org/10.1038/sdata.2017.122

Karger D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E, Linder, H.P., Kessler, M. (2018): Data from: Climatologies at high resolution for the earth’s land surface areas. EnviDat. https://doi.org/10.16904/envidat.228.v2.1

Krasting, John P.; John, Jasmin G; Blanton, Chris; McHugh, Colleen; Nikonov, Serguei; Radhakrishnan, Aparna; Rand, Kristopher; Zadeh, Niki T.; Balaji, V; Durachta, Jeff; Dupuis, Christopher; Menzel, Raymond; Robinson, Thomas; Underwood, Seth; Vahlenkamp, Hans; Dunne, Krista A.; Gauthier, Paul PG; Ginoux, Paul; Griffies, Stephen M.; Hallberg, Robert; Harrison, Matthew; Hurlin, William; Malyshev, Sergey; Naik, Vaishali; Paulot, Fabien; Paynter, David J; Ploshay, Jeffrey; Reichl, Brandon G; Schwarzkopf, Daniel M; Seman, Charles J; Silvers, Levi; Wyman, Bruce; Zeng, Yujin; Adcroft, Alistair; Dunne, John P.; Dussin, Raphael; Guo, Huan; He, Jian; Held, Isaac M; Horowitz, Larry W.; Lin, Pu; Milly, P.C.D; Shevliakova, Elena; Stock, Charles; Winton, Michael; Wittenberg, Andrew T.; Xie, Yuanyu; Zhao, Ming (2018). NOAA-GFDL GFDL-ESM4 model output prepared for CMIP6 CMIP. Version YYYYMMDD[1].Earth System Grid Federation. https://doi.org/10.22033/ESGF/CMIP6.1407

Maino, J. L., R. Schouten, J. C. Lye, P. A. Umina, and O. L. Reynolds. 2022. Mapping the life-history, development, and survival of spotted lantern fly in occupied and uninvaded ranges. Biological Invasions 24:2155–2167. 10.1007/s10530-022-02764-z

Weiss, D. J., A. Nelson, H. S. Gibson, W. Temperley, S. Peedell, A. Lieber, M. Hancher, et al. 2018. “A Global Map of Travel Time to Cities to Assess Inequalities in Accessibility in 2015.” Nature 553 (7688): 333–36. https://doi.org/10.1038/nature25181.


