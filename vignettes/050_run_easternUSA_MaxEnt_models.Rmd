---
title: "Run initial MaxEnt models used to choose background area (project step 1)"
author: "Samuel M. Owens"
contact: "sam.owens@temple.edu"
date: "2023-09-18"
output: html_document
---
# Overview

(THEORY)

# Setup

```{r load necesssary packages, echo = FALSE}

library(tidyverse)  #data manipulation

library(here) #making directory pathways easier on different instances
here()
# here() starts at the root folder of this package.
library(devtools)
library(rJava)

library(dismo) # runs maxent
library(ENMeval) # 

library(sf) # spatial data 
library(raster) # spatial data 
library(terra) # spatial data 
library(SDMtune)

```

```{r ggplot object for map style}

map_style <- list(
  xlab("longitude"),
  ylab("latitude"),
  theme_classic(),
  theme(legend.position = "bottom",
        panel.background = element_rect(fill = "lightblue",
                                colour = "lightblue")
        ),
  coord_equal() 
)

```

`Dismo` will run Maxent through java via the `rJava` package. You will need to ensure that your machine has the proper version of java installed (x32 or x64) for your operating system.

```{r}

checkMaxentInstallation(verbose = TRUE)

```

This chunk sets the java memory allocation (`Xmx`). I will start with 2GB memory. 

```{r control MaxEnt settings}

# xss sets java stack size
# xmx sets java memory allocation
options(java.parameters = "-Xmx2048m")

# options(java.parameters = c("-Xss2560k", "-Xmx2048m"))

```

# Setup for MaxEnt

- I may need to convert my slf points to a spatial points object

- IF I need to extract point-wise climatic data for MaxEnt, use these steps.

1. stack env covariates 
2. use slf pointwise data to extract a value from each layer at that each point

THIS IS A GOOD VALIDATION STEP- can use is.na()

First, I will load in the datasets I will need for the MaxEnt models. These are labeled at the beginning of the object name by the parameter they will be used for in the `maxent()` function. 

```{r load in files for all maxent models}

# path to directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/historical_climate_rasters/chelsa2.1_30arcsec")

# environmental and human impact covariates
# pull those of the proper ending
x_env_covariates_list <- list.files(path = file.path(mypath, "v1_maxent"), pattern = '\\_easternUSA.asc$', full.names = TRUE)



# slf presences
p_slf_points <- read.csv(file = file.path(here(), "vignette-outputs", "data-tables", "slf_all_final_coords_v1_2023-08.csv")) %>%
  dplyr::select(-species)

# background point sets
# entire eastern USA
a_entire_easternUSA_background_points <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "easternUSA_entire_flexible_area_points.csv"))
# DD model used with 1% adult emergence threshold
a_adult_emergence_1_background_points <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "easternUSA_adult_emergence-1_flexible_area_points.csv")) 
# 355km buffer around slf points
a_slf_buffered_background_points <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "easternUSA_slf_points_buffered_flexible_area_points.csv"))

```

I will load in the environmental covariates and stack them.

```{r stack env covariates}
 
# use in terra package first because stats are easier
x_env_covariates <- terra::rast(x = x_env_covariates_list)

# attributes
nlyr(x_env_covariates)
names(x_env_covariates)
minmax(x_env_covariates)
# ext(x_env_covariates)
# crs(x_env_covariates)

```

I will also reduce slf presences to just those within the eastern USA.

```{r}

# extent object for eastern USA
ext.obj <- terra::ext(-96.503906, -59.589844, 28.304381, 47.457809)

# conert to vector
p_slf_points_vect <- terra::vect(x = p_slf_points, geom = c("x", "y"), crs = "EPSG:4326") %>%
  # crop by extent area of interest
  terra::crop(., y = ext.obj) %>%
  # convert to geom, which gets coordinates of a spatVector
  terra::geom() 

# convert back to data frame
p_slf_points <- terra::as.data.frame(p_slf_points_vect) %>%
  dplyr::select(-c(geom, part, hole))

```

# Maxent Model- entire eastern USA

## create input data object

First, I will create an SWD (sample with data) object containing the presences and background points for this model

```{r prepare SWD data object}

entire_easternUSA_SWD <- SDMtune::prepareSWD(species = "Lycorma delicatula",
                                             env = x_env_covariates,
                                             p = p_slf_points,
                                             a = a_entire_easternUSA_background_points,
                                             verbose = TRUE # print helpful messages
                                             )

entire_easternUSA_SWD@coords # coordinates
entire_easternUSA_SWD@pa # presence / absence (background counted as absence)
entire_easternUSA_SWD@data # extracted data from 

```

```{r save SWD file}

SDMtune::swd2csv(swd = entire_easternUSA_SWD, file_name = c(
  file.path(here(), "vignette-outputs", "data-tables", "easternUSA_slf_presences_with_data.csv"),
  file.path(here(), "vignette-outputs", "data-tables", "easternUSA_entire_absences_with_data.csv")
  ))

```

## Create training / test split

I will split the presences into training and testing, using 80% of the points for training and 20% for testing. I will then use `SDMtune::randomFold()` to split the training data into 5 partitions for cross-validation. This method was loosely adapted from Srivastava et.al, 2021. 

```{r split data for training and testing}

entire_easternUSA_trainTest <-  SDMtune::trainValTest(
  x = entire_easternUSA_SWD,
  test = 0.2,
  only_presence = TRUE
)

# separate off training data
entire_easternUSA_train <- entire_easternUSA_trainTest[[1]]
entire_easternUSA_test <- entire_easternUSA_trainTest[[2]]

entire_easternUSA_train@coords # coordinates
entire_easternUSA_train@pa # presence / absence (background counted as absence)
entire_easternUSA_train@data # extracted data from

```

```{r split data into k folds}

# create random folds
entire_easternUSA_trainFolds <- SDMtune::randomFolds(
  data = entire_easternUSA_train,
  k = 5, # 5 folds
  only_presence = TRUE,
  seed = 4 # a truly random seed
)

```

## Train Maxent model

```{r create output directory for model}

mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models")

# create file directory
dir.create(path = file.path(mypath, "slf_easternUSA_entire_step1"))

```

```{r train maxent model}

entire_easternUSA_model <- SDMtune::train(
  method = "Maxent",
  data = entire_easternUSA_train,
  folds = entire_easternUSA_trainFolds, # 5 folds for dataset
  fc = "lqpht", # feature classes set to ALL
  iter = 5000, # number of iterations
  progress = TRUE
)

```

## Evaluate maxent model

```{r}

modelReport(
  model = entire_easternUSA_model,
  folder = file.path(mypath, "slf_easternUSA_entire_step1"),
  test = entire_easternUSA_test,
  type = "cloglog",
  response_curves = TRUE,
  jk = TRUE,
  verbose = TRUE
  )




plotJk()

maxentTh()

plotPred(map, lt = "", colorramp = NULL, hr = FALSE)
plotResponse()
auc(model)
plotROC(model)
tss(model)

```





# MaxEnt Model- DD 1% adult emergence

```{r create output directory for model}

```



# MaxEnt Model- 355km buffer around presences

```{r create output directory for model}

```





# References

Feng, X. 2022, April 24. shandongfx/nimbios_enm. GitHub. Accessed on 2023-9-18.

Steven J. Phillips, Miroslav Dudík, Robert E. Schapire. [Internet] Maxent software for modeling species niches and distributions (Version 3.4.4). Available from url: http://biodiversityinformatics.amnh.org/open_source/maxent/. Accessed on 2023-9-18.

Srivastava, V., A. D. Roe, M. A. Keena, R. C. Hamelin, and V. C. Griess. 2021. Oh the places they’ll go: improving species distribution modelling for invasive forest pests in an uncertain world. Biological Invasions 23:297–349.

Vignali, S., A. Barras, V. Braunisch, and C. B.-U. of Bern. 2023, July 3. SDMtune: Species Distribution Model Selection.


