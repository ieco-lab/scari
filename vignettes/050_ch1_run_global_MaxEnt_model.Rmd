---
title: "Run global MaxEnt model (chapter 1)"
author: "Samuel M. Owens"
contact: "sam.owens@temple.edu"
date: "2024-01-09"
output: html_document
---
# Overview

In the last vignette, I set up for the models I will run by cropping my chosen environmental covariates to the proper extents for both training and projecting the models. I also produced the background point datasets that are needed for MaxEnt to run.

In this vignette, I will (finally) start training MaxEnt models and obtaining the outputs I will need for my analysis! This vignette will train the `global` scale model and the next vignette will train the `regional` scale model. I begin by associating the presence data with the covariates in a data frame that will be fed into MaxEnt. I also separate these data into random folds for cross-validation. Once these are done, I can train the MaxEnt model. Outputs from this model will include a list of summary statistics, suitability maps and point-wise suitability values for all SLF presences and for important viticultural regions. 

This vignette has 3 subvignettes (051-053) that each outline and comment the basic workflow for the 3 R functions created for use in this vignette. These functions include `compute_MaxEnt_summary_statistics.R`, `create_MaxEnt_suitability_maps.R`, and `predict_xy_suitability.R`. Please see these subvignettes for a step-by-step guide to the creation of these functions. 

(THEORY)

# Setup

```{r load necesssary packages, echo = FALSE}

# general tools
library(tidyverse)  #data manipulation
library(here) #making directory pathways easier on different instances
here() # here() starts at the root folder of this package.
library(devtools)

# SDMtune and dependencies
library(SDMtune) # main package used to run SDMs
library(dismo) # package underneath SDMtune
library(rJava) # for running MaxEnt
library(plotROC) # plots ROCs

# spatial data handling
library(raster) 
library(terra) 

library(viridis)

```

`SDMtune` will run MaxEnt through java via the `rJava` package. You will need to ensure that your machine has the proper version of java installed (x32 or x64) for your operating system.

```{r check maxent installation}

checkMaxentInstallation(verbose = TRUE)

```

This chunk sets the java memory allocation (`Xmx`). I will increase the memory allocation from 512m (the default) to 2GB of memory. 

```{r control MaxEnt settings}

# xmx sets java memory allocation
options(java.parameters = "-Xmx2048m")

# xss sets java stack size
# options(java.parameters = c("-Xss2560k", "-Xmx2048m"))

```

# 1. Format Data for Global Model

## Create input datasets for modeling

I will load in the datasets I will need for the MaxEnt models. These are labeled at the beginning of the object name by the parameter they will be used for in the `maxent()` function (x, p or a). I will begin by loading in the covariate data and then by loading in the points datasets.

```{r load in historical covariates}

# path to directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/historical_climate_rasters/chelsa2.1_30arcsec")

# the env covariates used to train the model global
x_global_hist_env_covariates_list <- list.files(path = file.path(mypath, "v1_maxent_10km"), pattern = "\\.asc$", full.names = TRUE) %>%
    grep("bio2_1981-2010_global.asc|bio11_1981-2010_global.asc|bio12_1981-2010_global.asc|bio15_1981-2010_global.asc", ., value = TRUE)

# the env covariates used to project to N America
x_NAmerica_hist_env_covariates_list <- list.files(path = file.path(mypath, "v1_maxent_10km"), pattern = "\\_NAmerica.asc$", full.names = TRUE) %>%
  # dont include Access to cities
  grep(pattern = "atc_2015", value = TRUE, invert = TRUE)

```

```{r load in CMIP6 covariates}

# path to directory
  mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/future_climate_rasters/chelsa2.1_30arcsec/2041-2070_ssp370_GFDL")

# the env covariates for performing xy predictions for global slf and IVR points
x_global_CMIP6_env_covariates_list <- list.files(path = file.path(mypath, "v1_maxent_10km"), pattern = "\\_global.asc$", full.names = TRUE) %>%
  # dont include Access to cities
  grep(pattern = "atc_2015", value = TRUE, invert = TRUE)

# the env covariates for creating prediction maps for N America
x_NAmerica_CMIP6_env_covariates_list <- list.files(path = file.path(mypath, "v1_maxent_10km"), pattern = "\\_NAmerica.asc$", full.names = TRUE) %>%
  # dont include Access to cities
  grep(pattern = "atc_2015", value = TRUE, invert = TRUE)

```

In this chunk, I will load in the SLF presence dataset created in vignette 030 and the background points dataset created in vignette 040.

```{r load in points datasets}

# slf presences
p_slf_points <- read_rds(file = file.path(here(), "data", "slf_all_final_coords_2024-02-01.rds")) %>%
  dplyr::select(-species)

# entire eastern USA background point set
a_global_background_points <- read_rds(file = file.path(here(), "data", "global_background_points_v3.rds"))

```

I will create rasters of the environmental covariates, stack and gather summary statistics. I will also shorten their names and exclude possible operators from layer names (for example, using the dash symbol was found to interfere with SDMtune making predictions for tables downstream).

```{r naming object}

# layer name object. Check order of layers first
env_layer_names <- c("bio11", "bio12", "bio15", "bio2")

```


```{r stack historical covariates and make naming consistent}

# stack env covariates
x_global_hist_env_covariates <- terra::rast(x = x_global_hist_env_covariates_list)

# attributes
nlyr(x_global_hist_env_covariates)
names(x_global_hist_env_covariates)
minmax(x_global_hist_env_covariates)
# ext(x_global_hist_env_covariates)
# crs(x_global_hist_env_covariates)


# I will change the name of the variables because they are throwing errors in SDMtune
names(x_global_hist_env_covariates) <- env_layer_names
# confirmed- SDMtune doesnt like dashes in column names (it is read as a mathematical operation)

# NAmerica rasters
# stack env covariates
x_NAmerica_hist_env_covariates <- terra::rast(x = x_NAmerica_hist_env_covariates_list)

# attributes
nlyr(x_NAmerica_hist_env_covariates)
names(x_NAmerica_hist_env_covariates)
minmax(x_NAmerica_hist_env_covariates)
# ext(x_NAmerica_hist_env_covariates)
# crs(x_NAmerica_hist_env_covariates)

names(x_NAmerica_hist_env_covariates) <- env_layer_names

```

```{r stack CMIP6 covariates and make naming consistent}

# stack env covariates
x_global_CMIP6_env_covariates <- terra::rast(x = x_global_CMIP6_env_covariates_list)

# attributes
nlyr(x_global_CMIP6_env_covariates)
names(x_global_CMIP6_env_covariates)
minmax(x_global_CMIP6_env_covariates)
# ext(x_global_CMIP6_env_covariates)
# crs(x_global_CMIP6_env_covariates)

names(x_global_CMIP6_env_covariates) <- env_layer_names



# stack env covariates
x_NAmerica_CMIP6_env_covariates <- terra::rast(x = x_NAmerica_CMIP6_env_covariates_list)

# attributes
nlyr(x_NAmerica_CMIP6_env_covariates)
names(x_NAmerica_CMIP6_env_covariates)
minmax(x_NAmerica_CMIP6_env_covariates)
# ext(x_NAmerica_CMIP6_env_covariates)
# crs(x_NAmerica_CMIP6_env_covariates)

names(x_NAmerica_CMIP6_env_covariates) <- env_layer_names
  
```

```{r remove unneeded objects}

rm(x_global_hist_env_covariates_list)
rm(x_NAmerica_hist_env_covariates_list)
rm(x_global_CMIP6_env_covariates_list)
rm(x_NAmerica_CMIP6_env_covariates_list)

```


### create input data object

I will need to combine the above covariate, presence and background point datasets into a single dataset to feed into MaxEnt. This dataset will need to contain point-wise values for each of the predictor covariates at the presence and background point coordinates. `SDMtune` takes an SWD (sample with data) object for this purpose, containing the presences and background points with associated covariate data to be fed into the model, so I will create this now.

```{r prepare SWD data object}

if (FALSE) {

  global_SWD <- SDMtune::prepareSWD(species = "Lycorma delicatula",
                                    env = x_global_hist_env_covariates,
                                    p = p_slf_points,
                                    a = a_global_background_points,
                                    verbose = TRUE # print helpful messages
                                    )
  
  global_SWD@coords # coordinates
  global_SWD@pa # presence / absence (background counted as absence)
  global_SWD@data # extracted data from 

}

```

I usually look at how many records were dropped before I save it, because SDMtune gives an undefined warning about how many samples were discarded. In this case, we only lost 4 presence records, so this is acceptable. I will also save the output to be used later.

```{r save SWD file}

SDMtune::swd2csv(swd = global_SWD, file_name = c(
  file.path(here(), "vignette-outputs", "data-tables", "global_slf_presences_with_data_v3.csv"),
  file.path(here(), "vignette-outputs", "data-tables", "global_background_points_with_data_v3.csv")
  ))

```

### Create training / test split

I will split the presences into training and testing, using 80% of the points for training and 20% for testing. I will then use `SDMtune::randomFold()` to split the training data into 10 partitions for cross-validation. This method was loosely adapted from Srivastava et.al, 2021. 

Before settling on this method, I attempted a blocked cross-validation methodology using the package `blockCV`. I attempted it because many papers say that spatially segmenting the k-folds is a more rigorous method than simple random k-fold. However, this method did not work for my dataset and left many test folds with 0 points. I believe this is due to both the spatial scale I am working in for this model (global) and the comparatively narrow species range (limited to only east asia and the USA). The block size that reduces autocorrelation was about 1.5 million meters,  but this made the blocks too large for each block to receive adequate species presences. I also attempted spatial blocking by distance, but this yielded similar results. See `deprecated_code.Rmd` for code.

I will use 10-fold random cross validation.

```{r split data for training and testing}

global_trainTest <-  SDMtune::trainValTest(
  x = global_SWD,
  test = 0.2,
  only_presence = TRUE,
  seed = 3
)

# separate off training data
global_train <- global_trainTest[[1]]
global_test <- global_trainTest[[2]]

global_train@coords # coordinates
global_train@pa # presence / absence (background counted as absence)
global_train@data # extracted data from

```

```{r split data into k folds}

# create random folds
global_trainFolds <- SDMtune::randomFolds(
  data = global_train,
  k = 10, # 10 folds
  only_presence = TRUE,
  seed = 4
)

```

# 2. Train & Tune Global Model

## Intitial Training

First, I will train a model with 10 cross-validated replicates. The model will only be trained on 80% of the SLF presence data, with the other 20% being used downstream for analysis. The default settings will be used for MaxEnt, apart from the following changes:

* ALL feature types used (l = linear, q = quadratic, p = product, h = hinge, t = threshold)
* replicates = all final models will have 10 replicates
* iterations = 5000. This is the max number of iterations for the optimization algorithm to perform before stopping training. Increasing this number from the default of 500 allows the algorithm to make more refined predictions.

I will use these hyperparameters to train the initial model, but these parameters may change based on model tuning, which I will perform next.

```{r train maxent model}

global_model <- SDMtune::train(
  method = "Maxent",
  data = global_train,
  folds = global_trainFolds, # 5 folds for dataset
  fc = "lqpht", # feature classes set to ALL
  iter = 5000, # number of iterations
  progress = TRUE
)

```

## Tuning MaxEnt Model

I selected the hyperparameters that I thought were best for the model based on numerous papers. However, modeling papers for other species have made different choices for the parameter selection that I did not. Luckily, `SDMtune` makes it easy to select the best fit set of hyperparameters from a range of possibilities using the `gridSearch()` function. This function trains a different model for every possible combination of hyperparameters that is listed. I selected these hyperparameters for specific reasons listed in the literature.

Feature classes: h, qpht, lqpht
Feature classes are important because they govern how your environmental covariates are transformed by the algorithm (for example, if most presences fall beyond a specific point on the range of values, the algorithm might determine that a threshold feature is most appropriate.)

* Elith et.al, 2011 used only hinge features or all features and showed that hinge features alone may perform better. They referred to a MaxEnt model using only hinge features as resembling an additive model, not unlike a GAM. 
* The supplemental materials from the same paper demonstrated that linear features are redundant if also using hinge features.
* Most modeling papers, including Huron et.al and others for SLF, simply use all features.

Regularization Multiplier: 0.25, 0.5, 1, 1.5, 2
Regularization penalizes overfitting in the model. If a model is too fit to a range of covariate values, to the point that this model could not be applied to another area with other values, it receives a penalty. 

* Radosavljevic and Anderson 2014 used a range of regularization multipliers, from 0.25 to 2, to test for the most appropriate value.

```{r grid search tunable args}

if (TRUE) {
  
  # the arguments that can be tuned later
  getTunableArgs(global_model)
  
  # list of hyperparameters
  hypers <- list(
    reg = c(0.25, 0.5, 1, 1.5, 2), # regularization multiplier
    fc = c("h", "qpht", "lqpht") # feature classes
    )
  
  # Train a model with all possible combinations of hyperparameters
  global_model_tuned <- SDMtune::gridSearch(
    model = global_model, 
    hypers = hypers, # hyperparameter list
    metric = "auc", 
    save_models = TRUE,
    interactive = TRUE, 
    progress = TRUE
    )
  
  # show tuning results
  global_model_tuned@results
  # select number of best model
  best.model <- which.max(global_model_tuned@results$test_AUC)
  
}

```

Based on the model tuning, we see that the highest test AUC value was attributed to the model with a regularization multiplier of 1.5 and ALL features (model 12). We will need to re-train the model using these parameters.

## Re-training based on tuning

```{r train maxent model}

global_model <- SDMtune::train(
  method = "Maxent",
  data = global_train,
  folds = global_trainFolds, # 10 folds for dataset
  iter = 5000, # number of iterations
  fc = "lqpht", # all feature classes
  reg = 1.5, 
  progress = TRUE
)

```

## Summary Statistics

This function produces all summary statistics for this model. For the complete and annotated workflow used to create this function, see `051_maxent_workflow.Rmd`. 

```{r compute summary statistics using custom function}

# output directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_global_v2")

slfSpread::compute_MaxEnt_summary_statistics_CV(
  model.obj = global_model, 
  model.name = "global", 
  mypath = mypath, 
  create.dir = TRUE, # create subdirectory
  env.covar.obj = x_global_hist_env_covariates, # env covariates raster stacked
  train.obj = global_train, # training data used to create model
  trainFolds.obj = global_trainFolds,  # k-folds of training data
  test.obj = global_test, # data you wish to use to test the model
  jk.test.type = c("train", "test"), # types of jackknife curves to be created
  plot.type = c("cloglog", "logistic") # types of univariate and marginal response curves to be created
  )

```

I will now save the results from the model tuning that I performed a few chunks ago, because I have created the directory for these model results. I will also save the gridded result figure manually from the Viewer pane to the model directory folder that was created using `compute_MaxEnt_summary_statistics()`.

```{r save tuning results}

# save gridsearch interactive table from Viewer pane

# save df results of model tuning
write.csv(global_model_tuned@results, file = file.path(mypath, "global_model_tuning_results.csv"))

# save tuned models
write_rds(global_model_tuned, file = file.path(mypath, "global_models_tuned.rds"))

```

# 3. Create Outputs for Analysis

## Create distribution map for area of interest

Lastly, I will use the `SDMtune::predict()` function to predict the suitability for the range of interest. I will threshold the map by the `MTP`, `MTSS` and `10_percentile` thresholds. The workflow for this function can be found in vignette `052_create_suitability_maps_workflow.Rmd` and is wrapped into the function `slfSpread::create_MaxEnt_suitability_maps()`.

I will take the above map and reclassify it so that areas that aren't suitable are more obvious. I will use the `MTSS` (maximum training sensitivity plus specificity threshold), as this threshold is one of the most rigorous available (ADD MORE). I will also use the `MTP` (minimum training presence) and `10_percentile` (10 percentile training threshold) thresholds. These thresholds represent a continuum of training data exclusion. The `MTP` threshold does not exclude any data and assumes confidence in the validity of the training data, making the most conservative predictions (the largest suitable range for invasive species) The `10_percentile` threshold excludes the top 10% of suitable training samples, which would be more appropriate if I have less confidence in my training data. Finally, the `MTSS` threshold maximizes the sensitivity (the likelihood of detecting a true positive) and (WHAT ELSE) (Maryam Bordkhani (n.d.)).

I will retrieve the specific value for the MTSS threshold for this model and use it to define a what is considered unsuitable in this raster. Then, I will reclassify the mean raster so that anything below this threshold is now "unsuitable". I will use the mean cloglog threshold for all iterations. I will create a binary version of the mean raster, with unsuitable regions, which are below the MTSS training threshold, as 0, and those above the threshold as 1. Then, I can reclassify averaged raster of MaxEnt predictions. I will load in the averaged prediction for this model and reclassify the raster using a temporary raster layer that I will create. This raster will only have values where the model predicts the climate to be unsuitable, so that it can be plotted overtop the averaged raster to mask unsuitable areas

```{r set wd}

mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_global_v2")

```

```{r re-load model as needed}

if (FALSE) {
  
  global_model <- read_rds(file = file.path(mypath, "global_model.rds"))

}

```

```{r predict mean and thresholded suitability maps for historical data}

slfSpread::create_MaxEnt_suitability_maps_CV(
  model.obj = global_model,
  model.name = "global", 
  mypath = mypath, 
  create.dir = FALSE, 
  env.covar.obj = x_NAmerica_hist_env_covariates, 
  describe.proj = "NAmerica_1981-2010", # name of area or time period projected to
  clamp.pred = TRUE,
  map.thresh = TRUE, # whether thresholded versions of these maps should be created
  thresh = c("MTP", "10_percentile", "MTSS"),
  summary.file = file.path(mypath, "global_summary_all_iterations.csv")
)

```

I will also create global raster predictions.

```{r predict maps for globe}

slfSpread::create_MaxEnt_suitability_maps_CV(
  model.obj = global_model,
  model.name = "global", 
  mypath = mypath, 
  create.dir = FALSE, 
  env.covar.obj = x_global_hist_env_covariates, 
  describe.proj = "globe_1981-2010", # name of area or time period projected to
  clamp.pred = TRUE,
  map.thresh = TRUE, # whether thresholded versions of these maps should be created
  thresh = c("MTP", "10_percentile", "MTSS"),
  summary.file = file.path(mypath, "global_summary_all_iterations.csv")
)

```

I will also need to create a suitability map for the projected 2041-2070 climate data for N. America.

```{r predict normal and thresholded suitability maps for CMIP6 data}

slfSpread::create_MaxEnt_suitability_maps_CV(
  model.obj = global_model,
  model.name = "global", 
  mypath = mypath, 
  create.dir = FALSE, 
  env.covar.obj = x_NAmerica_CMIP6_env_covariates, 
  describe.proj = "NAmerica_2041-2070_GFDL_ssp370", # name of area or time period projected to
  clamp.pred = TRUE,
  map.thresh = TRUE, # whether thresholded versions of these maps should be created
  thresh = c("MTP", "10_percentile", "MTSS"),
  summary.file = file.path(mypath, "global_summary_all_iterations.csv")
)

```


## Predict suitability for all SLF presences

I will get projected suitability values, calculated on the cloglog scale (between 0 and 1) for each of the SLF presence points. These suitability values will be added back to the original data frame and saved. First, I will use `SDMtune::prepareSWD()` to extract raster values from each layer used to build the model. I will save this output.

During data analysis, I will create a scatter plot of these suitability values in the global vs regional models.

### Historical Data

```{r data import}

mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_global_v2")

# slf presence data
slf_presences <- read.csv(file = file.path(here(), "vignette-outputs", "data-tables", "slf_all_final_coords_2024-02-01.csv")) %>%
  dplyr::select(-species)

``` 

```{r predictions for SLF coordinate data}

slfSpread::predict_xy_suitability_CV(
  xy.obj = slf_presences,
  xy.type = "Lycorma delicatula",
  env.covar.obj = x_global_hist_env_covariates,
  model.obj = global_model,
  mypath = mypath,
  clamp.pred = TRUE, # clamp predictions
  predict.type = c("cloglog", "logistic"),
  output.name = "global_slf_all_coords_1981-2010"
)

```

### CMIP6 Data

I will repeat the same process for the CMIP6 data.

```{r predictions for SLF coordinate data CMIP6}

slfSpread::predict_xy_suitability_CV(
  xy.obj = slf_presences,
  xy.type = "Lycorma delicatula",
  env.covar.obj = x_global_CMIP6_env_covariates,
  model.obj = global_model,
  mypath = mypath,
  clamp.pred = TRUE,
  predict.type = c("cloglog", "logistic"),
  output.name = "global_slf_all_coords_2041-2070_GFDL_ssp370"
)

```

## Predict suibability for IVR regions

I will perform the same action as above, but for the locations of important wineries around the world. 

```{r data import}

mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_global_v2")

# load in all IVR points
load(file = file.path(here(), "data", "wineries.rda")) 

IVR_regions <- wineries %>%
  dplyr::select(x, y)

``` 

### Historical Data

```{r predictions for IVR coordinate data}

slfSpread::predict_xy_suitability_CV(
  xy.obj = IVR_regions,
  xy.type = "IVR regions",
  env.covar.obj = x_global_hist_env_covariates,
  model.obj = global_model,
  mypath = mypath,
  clamp.pred = TRUE,
  predict.type = c("cloglog", "logistic"),
  output.name = "global_wineries_1981-2010"
)

```

### CMIP6 Data

```{r predictions for SLF coordinate data CMIP6}

slfSpread::predict_xy_suitability_CV(
  xy.obj = IVR_regions,
  xy.type = "IVR regions",
  env.covar.obj = x_global_CMIP6_env_covariates,
  model.obj = global_model,
  mypath = mypath,
  clamp.pred = TRUE,
  predict.type = c("cloglog", "logistic"),
  output.name = "global_wineries_2041-2070_GFDL_ssp370"
)

```

Now I have all of the outputs I need from the global model! These outputs will be used in comparison with their regional version of their time period to analyze the realized regional niche for SLF at multiple spatial scales. These will be used to make inferences about the impacts of climate change on suitability for SLF.

In the next vignette, I will train the regional model needed for the spatial scale comparison and retrieve the data needed for the climate change comparison.

# References

Elith, J., S. J. Phillips, T. Hastie, M. Dudík, Y. E. Chee, and C. J. Yates. 2011. A statistical explanation of MaxEnt for ecologists: Statistical explanation of MaxEnt. Diversity and Distributions 17:43–57.

Feng, X. 2022, April 24. shandongfx/nimbios_enm. GitHub. Accessed on 2023-9-18.

Gallien, L., R. Douzet, S. Pratte, N. E. Zimmermann, and W. Thuiller. 2012. Invasive species distribution models – how violating the equilibrium assumption can create new insights. Global Ecology and Biogeography 21:1126–1136.

Maryam Bordkhani. (n.d.). threshold rule.

Radosavljevic, A., and R. P. Anderson. 2014. Making better Maxent models of species distributions: complexity, overfitting and evaluation. Journal of Biogeography 41:629–643.

Steven Phillips. (2017). A Brief Tutorial on Maxent. http://biodiversityinformatics.amnh.org/open_source/maxent/.

Steven J. Phillips, Miroslav Dudík, Robert E. Schapire. [Internet] Maxent software for modeling species niches and distributions (Version 3.4.4). Available from url: http://biodiversityinformatics.amnh.org/open_source/maxent/. Accessed on 2023-9-18.

Srivastava, V., A. D. Roe, M. A. Keena, R. C. Hamelin, and V. C. Griess. 2021. Oh the places they’ll go: improving species distribution modelling for invasive forest pests in an uncertain world. Biological Invasions 23:297–349.

VanDerWal, J., Shoo, L. P., Graham, C., & Williams, S. E. (2009). Selecting pseudo-absence data for presence-only distribution modeling: How far should you stray from what you know? Ecological Modelling, 220(4), 589–594. https://doi.org/10.1016/j.ecolmodel.2008.11.010

Vignali, S., A. Barras, V. Braunisch, and C. B.-U. of Bern. 2023, July 3. SDMtune: Species Distribution Model Selection.


