---
title: "slfrsk_maxent_modeling (slfrsk_practice_step_1-5)"
author: "Sam Owens"
date: "2022-08-03"
contact: "sam.owens@temple.edu"
output: html_document
---

This file will practice execution of the analyses outlined in the slfrsk research compendium, written by Nick Huron. The goal of this analysis is to help me to understand SDMs and their creation. I will be performing these analyses using a randomized and reduced subset of tinyslf (n = 500). 

File paths edited 2023-01-24, 2023-06-01

```{r load necesssary packages, echo = FALSE}

library(slfrsk) #this package, has extract_enm()
library(tidyverse)  #data manipulation

library(here) #making directory pathways easier on different instances
here()
# here() is "C:/Users/tun83449/OneDrive - Temple University/Shared drives/slfClimate/projects/slfSpread/slfSpread_pkg"
library(ENMTools) #enviro collinearity analyses
library(patchwork) #easy combined plots
library(scales) #rescale the plots easily
library(rgdal) #load shapefiles
library(doParallel) #parallized extraction
library(stringi)
library(terra)

```

# Reading and Visualizing Presence Data

```{r Read in cleaned gbif datasets}

slf_points_cleaned <- read_csv(file.path(here(), "data_root", "slf_points_cleaned_v0.csv"))

toh_points_cleaned <- read_csv(file.path(here(), "data_root", "toh_points_cleaned_v0.csv"))

```


```{r mapping slf and toh presence separately}

# these data frames were created by Nick and read in as the main data for the original tifs I (Sam) created with this script- Matt wants to see the differences between my data and Nick's, so I will need to run what I retrieved from gbif

# TOH
map_toh <- ggplot() +
  geom_polygon(data = map_data("world"), aes(x = long, y = lat, group = group), fill = "#FFFFFF", color = "black", lwd = 0.15) + # world map
  geom_point(data = toh_points_cleaned, aes(x = x, y = y), color = "#0072b2", alpha = 0.10) +
  theme_bw() +
  labs(x = "", y = "") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#f0f8ff")) +
  coord_quickmap(xlim = c(-164.5, 163.5), ylim = c(-55, 85)) +
  ggtitle(label = "TOH Presence")
  
# SLF
map_slf <- ggplot() +
  geom_polygon(data = map_data('world'), aes(x = long, y = lat, group = group), fill = "#FFFFFF", color = "black", lwd = 0.15) + #world map
  geom_point(data = slf_points_cleaned, aes(x = x, y = y), color = "#d55e00", alpha = 0.50) +
  theme_bw() +
  labs(x = "", y = "") +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        #plot.background = element_rect(fill ="#f0f8ff"),
        panel.background = element_rect(fill = "#f0f8ff")) +
  coord_quickmap(xlim = c(-164.5, 163.5), ylim = c(-55,85)) +
  ggtitle(label = "SLF Presence")
  
# patch maps together
map_toh / map_slf

# save maps
ggsave((map_toh / map_slf), filename = file.path(here(), "figures", "slf_toh_presence_2022.png"), height = 10, width = 10)

```

```{r plot together}

ggplot() +
  geom_polygon(data = map_data('world'), aes(x = long, y = lat, group = group), fill = "#FFFFFF", color = "black", lwd = 0.15) + #world map
geom_point(data = toh_points, aes(x = x, y = y), color = "#0072b2", alpha = 0.10) +
geom_point(data = slf_points, aes(x = x, y = y), color = "#d55e00", alpha = 0.50) +
  theme_bw() +
  labs(x = "", y = "") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        #plot.background = element_rect(fill ="#f0f8ff"),
        panel.background = element_rect(fill = "#f0f8ff")) +
  #theme_bw() +
  coord_quickmap(xlim = c(-164.5, 163.5), ylim = c(-55,85))

```

# Preparation of Spatial Data

```{r set raster extent}

if(FALSE){

  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/wc2.1_30arcsec")
  
  #ensure that extent is identical
#get file names
env.files <- list.files(file.path(mypath, "originals"), pattern = "[.]tif", full.names = T)
env.short <- list.files(file.path(mypath, "originals"), pattern = "[.]tif", full.names = F)

#change the labeling of the output layers
output.files <- env.short

#change weird BIOCLIM prefix
output.files <- gsub(pattern = "wc2.1_30s_bio", replacement = "global_bio", x = output.files)
#change weird ENVIREM prefix
#  <- gsub(pattern = "global", replacement = "global_env", x = output.files) #had to edit, assumed this was for forest height and elevation

#change weird ATC prefix
output.files <- gsub(pattern = "2015_accessibility_to_cities_v1.0", replacement = "global_atc", x = output.files)

#crop one of the BIOCLIM layers to set the bounding box and resolution to have files fixed
same.extent <- extent(-180, 180, -60, 84)
main_layer <- crop(raster(file.path(mypath,"originals", "wc2.1_30s_bio_1.tif")), y = same.extent, overwrite = F)

#reset extent stepwise
for(a in seq_along(env.files)){

  #ensure that the CRS is consistent
  rast.hold <- raster(env.files[a])
  crs(rast.hold) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
  #resample to fit the extent/resolution of the reference BIOCLIM layer
  #use bilinear interpolation, since values are continuous
  rast.hold <- resample(x = rast.hold, y = main_layer, method = "bilinear")

  #write out the new resampled rasters!
  writeRaster(x = rast.hold, filename = file.path(mypath,"v1", output.files[a]), overwrite = T)
}

} #closes no run if
  
```

```{r downsize raster resolution}

if(FALSE) {
  
  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/wc2.1_30arcsec")
  
  #list env layers, load
  env.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = T)
  env.short <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = F)
  # I had to modify "pattern" to not include the .tif.aux.xml files
  
  # downsampling by factor of 2 (read 2 cells deep around cell) and take mean of cells
  
  for(a in seq_along(env.files)){
    
    holder <- raster(env.files[a])
    down_holder <- raster::aggregate(holder, fact = 2, fun = mean, expand = TRUE, na.rm = TRUE, filename = file.path(mypath, "v1_downsampled", env.short[a]), overwrite = T)
    
  }
  
}

```

# Evaluation of Spatial Data Collinearity

```{r Take pearson correlation of raster layers}

if(FALSE){
  
  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/wc2.1_30arcsec")
  
  # load downsampled layers and stack for raster.cor.matrix command
  
  # list of layer paths
  env.files <- list.files(path = file.path(mypath, "v1_downsampled"), pattern = "\\.tif$", full.names = T)
  
  # stack downsampled layers
  env <- raster::stack(env.files)
  
  # evaluate correlations for raster layers
  # create a correlation matrix for picking model layers
  env.corr <- ENMTools::raster.cor.matrix(env, method = "pearson")
  
  # write out correlations as .csv
  write.csv(x = env.corr, file = file.path(here(), "data_raw/env_cor_v1_2022_downsampled.csv"), col.names = TRUE, row.names = TRUE)
  
}

```

```{r tidy env_cor_v1}

env.corr_2 <- read_csv(file = file.path(here(), "data_root/data_old/env_cor_v2_2022_downsampled.csv"))
# in this file, numbers of climatic variables were manually changed to include leading 0's (01 instead of 1)

env.corr_2_tidy <- as.data.frame(env.corr_2) 

row.names(env.corr_2_tidy) <- env.corr_2_tidy$...1
# read in data and make 1st column rownames

env.corr_2_tidy <- env.corr_2_tidy[, which(names(env.corr_2_tidy) != "...1")]

env.corr_2_tidy <- env.corr_2_tidy %>%
  select(order(colnames(env.corr_2_tidy))) %>%
  abs(.)

write.csv(x = env.corr_2_tidy, file = file.path(here(), "data_root/env_cor_v4_abs_2022_downsampled.csv"), col.names = TRUE, row.names = TRUE)
# row names will be re-ordered in excel

```


```{r visualize correlations as heatmap}

# re-read in correlation table 
env.cor <- read.csv(file = file.path(here(), "data_root/env_cor_v4_abs_2022_downsampled.csv"))
# v3 not used because abs value of correlations not taken

row.names(env.cor) <- env.cor$X
# first column is rowname

# make correlations into absolute values and tidy data for plotting
p_env_cor <- env.cor[, which(names(env.cor) != "X")] %>%
  abs(.) %>%
  as_tibble() %>%
  mutate(covar = colnames(.)) %>%
  dplyr::select(covar, everything()) %>%
  pivot_longer(cols = -covar, names_to = "var") %>%
  dplyr::select(var, covar, everything()) %>%
  ggplot() +
  geom_tile(aes(x = var, y = covar, fill = value)) +
  viridis::scale_fill_viridis(discrete = FALSE, direction = -1, limits = c(0,1), name = "abs(Correlation)") +
  guides(x =  guide_axis(angle = 90)) +
  labs(x = "", y = "")

p_env_cor

ggsave(p_env_cor, filename = file.path(here(), "figures", "p_env_cor2022_v4.png"), height = 10, width = 10)

```


```{r conversion of .tif to .asc files}

if(FALSE){
  
  mypath <- file.path(here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/wc2.1_30arcsec")
  
  # get/set file names
  env.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = T) 
  
 # env.files.1 <- env.files[!env.files %in% c("G:/Shared drives/slfModeling/projects/slfSpread/slfSpread_pkg/maxent_raw_env2022/v1/global_atc.tif",
  #                                           "G:/Shared drives/slfModeling/projects/slfSpread/slfSpread_pkg/maxent_raw_env2022/v1/global_bio_1.tif",
   #                                          "G:/Shared drives/slfModeling/projects/slfSpread/slfSpread_pkg/maxent_raw_env2022/v1/global_bio_10.tif",
    #                                         "G:/Shared drives/slfModeling/projects/slfSpread/slfSpread_pkg/maxent_raw_env2022/v1/global_bio_11.tif")]
  # subset of env.files containing first entry- to see file size
  
  env.short <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = F)
  
#  env.short.1 <- env.short[!env.short %in% c("global_atc.tif", "global_bio_1.tif", "global_bio_10.tif", "global_bio_11.tif")]
  # subset of env.files containing first entry- to see file size
  
  env.asc <- gsub(pattern = ".tif", replacement = ".asc", x = env.short)
  
 # env.asc.1 <- env.asc[!env.asc %in% c("global_atc.asc", "global_bio_1.asc", "global_bio_10.asc", "global_bio_11.asc")]
  # a copy to try only one entry of the list
  
  # loop to convert, set NAs equal to -9999 as required by Maxent
for(a in seq_along(env.files)){
    
  file_to_asc <- raster(env.files[a])
  NAvalue(file_to_asc) <- -9999
  writeRaster(x = file_to_asc, filename = file.path(mypath, "v1_maxent", env.asc[a]), format = "ascii", overwrite = F)
    
  }
  
   # file_to_asc <- env.files[1] %>%
    #  raster(.) %>%
     # NAvalue(.) <- -9999 %>%
    #  writeRaster(x = file_to_asc, filename = file.path(mypath, "v1_maxent", env.asc[1]), format = "ascii", overwrite = F)
    # just do 1 to start and see how large it is
}

```

# Building and evaluating Maxent SDMs

Maxent was used under the default parameters, excluding the following changes:

1. all feature types made available but set to AUTO FEATURES (linear, quadratic, product, threshold and hinge set to TRUE before setting auto features to TRUE)

2. Creative response curves set to TRUE

3. Do jackknife to measure variable importance set to TRUE

4. Replicates set to 5 for SLF 
    (this is # of k-fold crossvalidation replicates and determines test proportion from k), 10 for TOH
    
5. Apply threshold rule set to MINIMUM training presence

6. Threads set to available number of processor threads available

## At this point, run model in MaxEnt



# Cropping most used bioclim layers to N America bounding box and visualizing

Library necessary packages

```{r library packages}

library(terra) # most of analyses done here
library(raster) # used some for rasters
library(sf) # dependency of terra
library(tidyverse)
library(here) # pathing

```

```{r cropping bioclim layers to N america}

if(FALSE) {

  # path to directory
  mypath <- file.path(here::here() %>% 
                        dirname(),
                      "maxent/historical_climate_rasters/wc2.1_30arcsec")

  # load in bioclim layers to be cropped- the original .tif files
  env.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = T) %>%
  # extract the 6 bioclim layers (plus global_atc) that I expect to use in future regional models
    grep("global_atc.tif|bio_1.tif|bio_2.tif|bio_7.tif|bio_12.tif|bio_15.tif", ., value = TRUE)

   # output file names
  output.files <- list.files(path = file.path(mypath, "v1"), pattern = "\\.tif$", full.names = F) %>%
  # extract the 6 bioclim layers (plus global_atc) that I expect to use in future regional models
    grep("global_atc.tif|bio_1.tif|bio_2.tif|bio_7.tif|bio_12.tif|bio_15.tif", ., value = TRUE) %>%
    # get rid of filetype endings
    gsub(pattern = ".tif", replacement = "") 
  
  
  # define extent object
  NAmerica_extent <- terra::ext(-133.593750, -52.294922, 25.085599, 55.304138)
  
  # create main layer to crop others to
  main_layer <- terra::crop(terra::rast(x = env.files[2]),
                            # crop global_bio_1 to extent
                            y = NAmerica_extent, overwrite = FALSE)
  
  # ensure crs is correct
  crs(main_layer, describe = TRUE)
  
  # crs isnt correct. will set in loop
  
  
  
  # convert to df for plotting 
  main_layer_df <- terra::as.data.frame(main_layer, xy = TRUE)
  
  # plot main layer as example
  main_plot <- ggplot() +
    # change scale of plots to be standard across figures
    geom_raster(data = main_layer_df, 
              aes(x = x, y = y, fill = global_bio_1)) +
    # set view to N America extent, plus 2 on each side to see if cropping worked
    coord_cartesian() +
    xlab("longitude") +
    ylab("latitude") +
    theme_minimal() +
    theme(legend.position = "none")
  # the cropping worked
  
  # view list of filetypes for terra, use .ascii
  View(terra::gdal(drivers = TRUE))
  
  # loop to crop extent for all files
  for(a in seq_along(env.files)){

    #ensure that the CRS is consistent
    rast.hold <- terra::rast(env.files[a])
    crs(rast.hold) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
    #resample to fit the extent/resolution of the reference BIOCLIM layer
    #use bilinear interpolation, since values are continuous
    rast.hold <- terra::resample(x = rast.hold, y = main_layer, method = "bilinear")
    
    # set NA value, a requirement of maxent .asc files
    rast.hold <- terra::subst(rast.hold, NA, -9999)
    
    #write out the new resampled rasters!
    terra::writeRaster(x = rast.hold, filename = file.path(mypath, "v1_maxent", paste0(output.files[a], "_NAmerica")), filetype = "AAIGrid", overwrite = TRUE)
    
  }
  
}

```


```{r visualize new layers and MaxEnt output from slf_gbif_v5}

# path to directory
  mypath <- file.path(here::here() %>% 
                        dirname(),
                      "maxent/maxent_models")

# load in global model
v5_global <- terra::rast(x = file.path(mypath, "maxent_slf_gbif_v5_global", "Lycorma_delicatula_avg.asc")) %>%
  # crop to only area of interest because entire raster too long (almost impossible) to convert to df
  crop(., y = NAmerica_extent)

# do a quick plot
terra::plot(v5_global)

# convert to df to plot with other layers
v5_global_df <- terra::as.data.frame(v5_global, xy = TRUE) %>%
  # remove NAs
  na.omit()

# plot
v5_global_plot <- ggplot() +
  geom_raster(data = v5_global_df, 
              aes(x = x, y = y, fill = Lycorma_delicatula_avg)) +
  # keep lat/long ratio equal when plotting
  coord_equal() +
  xlab("longitude") +
  ylab("latitude") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Predicted current global niche for Lycorma delicatula")



# load in regional model
v5_regional <- terra::rast(x = file.path(mypath, "maxent_slf_gbif_v5_regional", "Lycorma_delicatula_avg.asc"))

# quick plot
terra::plot(v5_regional)

# convert to df
v5_regional_df <- terra::as.data.frame(v5_regional, xy = TRUE) %>%
  na.omit()

# plot
v5_regional_plot <- ggplot() +
  geom_raster(data = v5_regional_df, 
              aes(x = x, y = y, fill = Lycorma_delicatula_avg)) +
  coord_equal() +
  xlab("longitude") +
  ylab("latitude") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Predicted current regional niche for Lycorma delicatula")

```

# Analyzing differences in regional vs global model outputs

For my analysis, I need to compare the predictions of both the regional and global models in the invaded region. First, I will need to infer the amount of range filling by SLF according to each model. In Gallien et.al, this was done by plotting the points overtop of the raster of both the regional and global models and comparing the models for the level of agreement in suitability. There are a few steps to this process:

1. Convert each output to binary (suitable/unsuitable) based on a suitability threshold (likely the threshold rule chosen in MaxEnt, MTP), but could also be 0.5 bc Gallien et.al used this.
2. Overlay the regional and global model predictions
3. Average rasters
4. Re-classify averaged raster as areas where both agree on presence, both agree on absence, or one classifies and the other classifies absent (4 total categories)

Second, I will need to compare the performance of each model at each point across the invaded range. Gallien et.al did this for both observed presences and pseudo-absences ("background points"). This was used to produce a boxplot of predicted suitability in the global vs the regional model. This process will also take a few steps:

1. Start with overlaid binary suitability regional and global maps
2. Extract point-wise suitability for presences and absences in each model (4 categories)
3. Graph these suitability values- x-axis of plot is suitability in global model, y-axis is suitability in regional model

Finally, these values can also be used to calculate the sensitivity and specificity of each model manually.

1. Calculate sensitivity (proportion of presences correctly guessed) for each model
2. calculate specificity (proportion of absences correctly guessed) for each model

## infer level of range filling

First, I will create matrix objects that will be used to reclassify the output rasters to a binary predictor of suitability. 0.5 will be the threshold: anything above 0.5 will be considered suitable, while anything below will not be. This threshold is essential for my invasion stage anaylsis further downstream.

To reclassify, package terra takes a 3-column matrix for its "classify" function, which re-classifies groups of values to other values. I will create one for the global model output and one for the regional model output. The reason for having two separate matrices is because I will need to differentiate between a "suitable" spot on the global and regional rasters. Because of this, I will be encoding the rasters into base-2 binary so that the end result is 4 categories of values (the idea for this was found in this [forum](https://gis.stackexchange.com/questions/127055/comparing-and-finding-inaccuracies-in-two-raster-layers)). For the regional raster, I will encode the logistical output of MaxEnt into these values:

*Regional unsuitable = 0-0.5 -> 00000001 = 1
*Regional suitable = 0.51-1 ->  00000010 = 2

For the global rasters, I will encode the outputs into these values:

*Global unsuitable = 0-0.5 -> 00000100 = 4
*Global suitable = 0.51-1 ->  00001000 = 8

By encoding the rasters into base-2, I have created only 4 possible values of the raster layers, which can be translated into 4 categories:

*Unsuitable Agreement = 5 (00000101)
*Suitable Agreement = 10 (00001010)
*unsuitable regional/suitable global = 9 (00001001)
*Suitable regional/unsuitable global = 6 (00000110)

### 1. convert output rasters to binary predictors

To convert the rasters to binary predictors of presence / absence, first I will create the classification matrices needed by the \code(terra) package.

```{r }

# create regional suitability value matrix for terra
regional_classes <- data.frame(
  from = c(0, 0.51),
  to = c(0.50, 1),
  becomes = c(strtoi(00000001, base = 2), strtoi(00000010, base = 2)) # the strtoi function converts to base-2
)

global_classes <- data.frame(
  from = c(0, 0.51),
  to = c(0.50, 1),
  becomes = c(00000100, 00001000)
)


```

Next, 






```{r}
# this creates a spatRaster object with 2 layers
v5_global_regional <- c(v5_regional, v5_global)
nlyr(v5_global_regional)



# plot both maps together
v5_global_regional_plot <- ggplot() +
  geom_raster(data = v5_global_df, 
              aes(x = x, y = y, fill = Lycorma_delicatula_avg)) +
  geom_raster(data = v5_regional_df, 
              aes(x = x, y = y, fill = Lycorma_delicatula_avg)) +
  coord_equal() +
  xlab("longitude") +
  ylab("latitude") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Predicted niche overlap for Lycorma delicatula")

```



