---
title: "Run initial MaxEnt models used to choose background area (project step 1)"
author: "Samuel M. Owens"
contact: "sam.owens@temple.edu"
date: "2023-09-18"
output: html_document
---
# Overview

(THEORY)

# Setup

```{r load necesssary packages, echo = FALSE}

library(tidyverse)  #data manipulation

library(here) #making directory pathways easier on different instances
here()
# here() starts at the root folder of this package.
library(devtools)
library(rJava) # for running MaxEnt

library(dismo) # package underneath SDMtune
library(SDMtune) # main package used to run SDMs

# dependencies of SDMtune
library(plotROC) # plots ROCs

library(viridis)

# spatial data handling
library(raster) 
library(terra) 

library(pROC)

```

`SDMtune` will run Maxent through java via the `rJava` package. You will need to ensure that your machine has the proper version of java installed (x32 or x64) for your operating system.

```{r check maxent installation}

checkMaxentInstallation(verbose = TRUE)

```

This chunk sets the java memory allocation (`Xmx`). I will increase the memory allocation from 512m (the default) to 2GB of memory. 

```{r control MaxEnt settings}

# xss sets java stack size
# xmx sets java memory allocation
options(java.parameters = "-Xmx2048m")

# options(java.parameters = c("-Xss2560k", "-Xmx2048m"))

```

```{r ggplot object for map style}

map_style <- list(
  xlab("longitude"),
  ylab("latitude"),
  theme_classic(),
  theme(legend.position = "bottom",
        panel.background = element_rect(fill = "lightblue",
                                colour = "lightblue")
        ),
  coord_equal() 
)

```

# MaxEnt Model- entire eastern USA, with "access to cities" covariate

## Tidy datasets for modeling

First, I will load in the datasets I will need for the MaxEnt models. These are labeled at the beginning of the object name by the parameter they will be used for in the `maxent()` function. 

```{r load in files for all maxent models}

# path to directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/historical_climate_rasters/chelsa2.1_30arcsec")


# environmental and human impact covariates.
# the env covariates used to train the model
x_env_covariates_list <- list.files(path = file.path(mypath, "v1_maxent"), pattern = '\\_easternUSA.asc$', full.names = TRUE)

# these layers will take a long time to load so only do it if you need the global versions
if (FALSE) {
  
  # the same covariates, but on a global scale (used to make predictions)
  x_global_env_covariates_list <- list.files(path = file.path(mypath, "v1_maxent"), pattern = "\\.asc$", full.names = TRUE) %>%
    grep("atc_2015_global.asc|bio2_1981-2010_global.asc|bio11_1981-2010_global.asc|bio12_1981-2010_global.asc|bio15_1981-2010_global.asc", ., value = TRUE)

}

# slf presences
p_slf_points <- read.csv(file = file.path(here(), "vignette-outputs", "data-tables", "slf_all_final_coords_v1_2023-08.csv")) %>%
  dplyr::select(-species)

# entire eastern USA background point set
a_entire_easternUSA_background_points <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "easternUSA_entire_flexible_area_points.csv"))

```

I will load in the environmental covariates and stack them. I will also shorten their names and exclude possible operators from layer names (for example, using the dash symbol was found to interfere with SDMtune's ability to make predictions for tables downstream).

```{r stack env covariates and make naming consistent}

# stack env covariates
x_env_covariates <- terra::rast(x = x_env_covariates_list)
# stack global versions
x_global_env_covariates <- terra::rast(x = x_global_env_covariates_list)

# attributes
nlyr(x_env_covariates)
names(x_env_covariates)
minmax(x_env_covariates)
# ext(x_env_covariates)
# crs(x_env_covariates)


# layer name object
env_layer_names <- c("atc_2015", "bio11_1981_2010", "bio12_1981_2010", "bio15_1981_2010", "bio2_1981_2010")



# I will change the name of the variables because they are throwing errors in SDMtune
names(x_env_covariates) <- env_layer_names
# confirmed- SDMtune doesnt like dashes in column names (it is read as a mathematical operation)

# rename raster layers to be the same as the input layers
names(x_global_env_covariates)
# I will change the name of the variables because they are throwing errors in SDMtune
names(x_global_env_covariates) <- env_layer_names

```

I will make some quick edits to the SLF presence data as well. These need to be cropped to the extent of the the background area (in this case, the Eastern USA).

```{r crop presences to rasters}

# extent object for eastern USA
ext.obj <- terra::ext(-96.503906, -59.589844, 28.304381, 47.457809)

# conert to vector
p_slf_points_vect <- terra::vect(x = p_slf_points, geom = c("x", "y"), crs = "EPSG:4326") %>%
  # crop by extent area of interest
  terra::crop(., y = ext.obj) %>%
  # convert to geom, which gets coordinates of a spatVector
  terra::geom() 

# convert back to data frame
p_slf_points <- terra::as.data.frame(p_slf_points_vect) %>%
  dplyr::select(-c(geom, part, hole))

```

Finally, I will create an output directory folder to hold the model and its output data.

```{r create directory for file}

mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models")

# create directory for model
dir.create(path = file.path(mypath, "slf_easternUSA_entire_step1"))
# create plots subdir
dir.create(path = file.path(mypath, "slf_easternUSA_entire_step1", "plots"))

```

### create input data object

I need to create a dataset of presences and background points. This dataset will need to contain point-wise values for each of the predictor covariates. `SDMtune` takes an SWD (sample with data) object for this purpose, containing the presences and background points with associated covariate data to be fed into the model.

```{r prepare SWD data object}

if (FALSE) {

  entire_easternUSA_SWD <- SDMtune::prepareSWD(species = "Lycorma delicatula",
                                               env = x_env_covariates,
                                               p = p_slf_points,
                                               a = a_entire_easternUSA_background_points,
                                               verbose = TRUE # print helpful messages
                                               )
  
  entire_easternUSA_SWD@coords # coordinates
  entire_easternUSA_SWD@pa # presence / absence (background counted as absence)
  entire_easternUSA_SWD@data # extracted data from 

}

```

I will also save the output to be used later.

```{r save SWD file}

SDMtune::swd2csv(swd = entire_easternUSA_SWD, file_name = c(
  file.path(here(), "vignette-outputs", "data-tables", "easternUSA_slf_presences_with_data.csv"),
  file.path(here(), "vignette-outputs", "data-tables", "easternUSA_entire_absences_with_data.csv")
  ))

```

### Create training / test split

I will split the presences into training and testing, using 80% of the points for training and 20% for testing. I will then use `SDMtune::randomFold()` to split the training data into 5 partitions for cross-validation. This method was loosely adapted from Srivastava et.al, 2021. 

```{r split data for training and testing}

set.seed(4)

entire_easternUSA_trainTest <-  SDMtune::trainValTest(
  x = entire_easternUSA_SWD,
  test = 0.2,
  only_presence = TRUE
)

# separate off training data
entire_easternUSA_train <- entire_easternUSA_trainTest[[1]]
entire_easternUSA_test <- entire_easternUSA_trainTest[[2]]

entire_easternUSA_train@coords # coordinates
entire_easternUSA_train@pa # presence / absence (background counted as absence)
entire_easternUSA_train@data # extracted data from

```

```{r split data into k folds}

# create random folds
entire_easternUSA_trainFolds <- SDMtune::randomFolds(
  data = entire_easternUSA_train,
  k = 5, # 5 folds
  only_presence = TRUE,
  seed = 5 
)

```

## Train Maxent model

First, I will train a model with 5 cross-validated replicates. The model will only be trained on 80% of the slf presence data, with the other 20% being used downstream for analysis. The default settings will be used for Maxent, apart from the following changes:

* ALL feature types used (l = linear, q = quadratic, p = product, h = hinge, t = threshold)
* replicates = 5
* iterations = 5000. This is the max number of iterations of optimization algorithm to perform before stopping training. Increasing this number allows the algorithm to make more refined predictions.

Other changes to the default will be used, but are not relevant for training the model.

```{r train maxent model}

entire_easternUSA_model <- SDMtune::train(
  method = "Maxent",
  data = entire_easternUSA_train,
  folds = entire_easternUSA_trainFolds, # 5 folds for dataset
  fc = "lqpht", # feature classes set to ALL
  iter = 5000, # number of iterations
  progress = TRUE
)

```

## Evaluate maxent model

This function should produce all summary statistics for this model

```{r compute summary statistics using custom function}

slfSpread::compute_MaxEnt_summary_statistics(
  model.obj = entire_easternUSA_model, 
  model.name = "entire_easternUSA_model", 
  mypath = mypath, 
  env.covar.obj = x_env_covariates, 
  train.obj = entire_easternUSA_train, 
  trainFolds.obj = entire_easternUSA_trainFolds, 
  test.obj = entire_easternUSA_test, 
  plot.types = c("train", "test"), 
  threshold.types = c("cloglog", "logistic")
  )

```

### Fixed area AUC

In order to calculate the ROC values for fixed vs flexible AUC, I need to extract the predicted suitability for the test data that were separated off and for the background points that were selected from a regular grid across the study area. Some of this method was taken from Phillips et.al, 2017.

First, I will load in the test dataset and make predictions for only those presences.

```{r load test dataset}

entire_fixed_presences <- read.csv(file = file.path(mypath, "easternUSA_entire_test.csv")) %>%
  dplyr::filter(pa == 1) %>%
  dplyr::select(atc_2015:bio2_1981_2010)

```

```{r make predictions for test dataset}

entire_fixed_presences_predict <- SDMtune::predict(
  object = entire_easternUSA_model, # model
  data = entire_fixed_presences, # data for prediction
  fun = "mean", # function to be applied
  type = "cloglog", # default for MaxEnt
  clamp = FALSE, # dont do clamping to restrict predictions
  progress = TRUE # progress bar
) %>%
  as.data.frame() %>%
  rename("cloglog_suitability" = ".")

```

```{r bind rows and write to csv}

# read in dataset again
entire_fixed_presences <- read.csv(file = file.path(mypath, "easternUSA_entire_test.csv")) %>%
  dplyr::filter(pa == 1) %>%
  dplyr::select(Species, X, Y)

# bind rows
entire_fixed_presences <- cbind(entire_fixed_presences, entire_fixed_presences_predict) 
  
entire_fixed_presences <-   dplyr::rename(entire_fixed_presences, "x" = "X", "y" = "Y")
# output directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_step1")
# save output
write_csv(x = entire_fixed_presences, file = file.path(mypath, "entire_easternUSA_predicted_suitability_test_presences.csv"))

```

Next, I will perform the same procedure for the fixed background points. I will also need to extract the raster values at each point to perform this calculation, which could take quite awhile. 

```{r load fixed background points and extract raster values}

a_entire_fixed_background <- read.csv(file = file.path(here(), "vignette-outputs", "data-tables", "easternUSA_fixed_area_points_v1.csv")) %>%
  dplyr::select(-cell) 

if (TRUE) {

  # get SWD object containing point location data from rasters
  a_entire_fixed_background <- SDMtune::prepareSWD(species = "Lycorma delicatula",
                                                   env = x_env_covariates, 
                                                   a = a_entire_fixed_background,
                                                   verbose = TRUE 
                                                   )
  
#  a_entire_fixed_background <- terra::extract(
#    x = x_env_covariates, 
#    y = a_entire_fixed_background, 
 #   xy = TRUE, 
 #   method = "simple", 
#    bind = TRUE
#    )
                                       
}

```

```{r make predictions for background dataset}

entire_fixed_background_predict <- SDMtune::predict(
  object = entire_easternUSA_model, # model
  data = a_entire_fixed_background, # data for prediction
  fun = "mean", # function to be applied
  type = "cloglog", # default for MaxEnt
  clamp = FALSE, # dont do clamping to restrict predictions
  progress = TRUE # progress bar
) %>%
  as.data.frame() %>%
  rename("cloglog_suitability" = ".")

```

```{r bind rows and write to csv}

# read in dataset again
entire_fixed_background <- read.csv(file = file.path(here(), "vignette-outputs", "data-tables", "easternUSA_fixed_area_points_v1.csv")) %>%
  dplyr::select(-cell)

# bind rows
entire_fixed_background <- cbind(entire_fixed_background, entire_fixed_background_predict)
# output directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_step1")
# save output
write_csv(x = entire_fixed_background, file = file.path(mypath, "entire_easternUSA_predicted_suitability_fixed_background.csv"))

```

These data frames need to be combined into one data frame with 3 columns: the type (label of test vs background point), the responses (whether a point is 1 for a test presence or 0 for a background point), and the predictors (the predicted suitability values). I will be using the `pROC` package to calculate an ROC. 

Finally, I will create an ROC curve for the fixed area AUC and test presences and calculate an AUC from it.

```{r format data for ROC curve}

# add response columns (pb)
entire_fixed_presences <- dplyr::mutate(entire_fixed_presences, pb = 1) %>%
  # also drop species name
  dplyr::select(-Species)
entire_fixed_background <- dplyr::mutate(entire_fixed_background, pb = 0)

# join datasets
entire_fixed_ROC_df <- full_join(entire_fixed_presences, entire_fixed_background, by = c("x", "y", "pb", "cloglog_suitability")) %>%
  dplyr::select(-c(x, y))

```

```{r create ROC curve}

# set plot type for graphics to square
#par(pty = "s")

# calculate ROC object
entire_fixed_ROC <- pROC::roc(
  response = entire_fixed_ROC_df$pb, # the actual presence or background data
  predictor = entire_fixed_ROC_df$cloglog_suitability, # the predictions made by the model
  )

# calculate AUC as object
entire_fixed_ROC_AUC <- pROC::auc(roc = entire_fixed_ROC)
# isolate AUC numeric value
entire_fixed_ROC_AUC_value <- as.numeric(print(entire_fixed_ROC_AUC)) %>%
  round(., digits = 3)

# plot
#entire_fixed_ROC_plot <- pROC::plot.roc(
#  x = entire_fixed_ROC,
#  print.auc = TRUE,
#  print.auc.x = 0.45, # print AUC at x = 45 location
#  # graphics settings
#  legacy.axes = TRUE, # makes x axis 1-specificity
#  xlab = "True Positive Rate",
#  ylab = "False Positive Rate",
#  main = "entire_easternUSA_model fixed area ROC curve and AUC",
#  col = "red2",
#  lwd = 2,
#  asp = 1
#)

# plot as ggplot object
entire_fixed_ROC_ggplot <- pROC::ggroc(
  data = entire_fixed_ROC,
  legacy.axes = TRUE, # makes x axis 1-specificity
  colour = "red2",
  linewidth = 1
) +
  geom_abline(slope = 1,
              linetype = 2) + # add 0.5 null model line
  theme_bw() +
  theme(aspect.ratio = 1,
        legend.position = "right") +
  xlab("True Positive Rate") +
  ylab("False Positive Rate") +
  ggtitle("entire_easternUSA_model fixed area ROC curve and AUC") +
  annotate(geom = "text",
          x = 0.75,
          y = 0.25,
          label = paste0("Fixed Area AUC:\n", entire_fixed_ROC_AUC_value)
          )

```

```{r save ROC curve}
# output dorectory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_step1")

# save plot
ggsave(
  filename = file.path(mypath, "plots", "entire_easternUSA_model_fixed_background_ROC.jpg"),
  plot = entire_fixed_ROC_ggplot,
  device = "jpeg",
  height = 8,
  width = 10,
  dpi = "retina"
  )

# close device
#dev.off()
# reset plotting settings
#par(pty = "m")

```

### Flexible vs Fixed area AUC plots

I will export the AUC values for the fixed vs flexible background points as a data frame. These will be used later to create a plot for the fixed and flexible area AUCs for all 3 models, in order to discern the best background size.

```{r Fixed and flexible AUC}

# output dorectory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_step1")

# redefine AUC object
entire_fixed_ROC_AUC_value <- as.numeric(print(entire_fixed_ROC_AUC)) %>%
  round(., digits = 7)

entire_AUC_fixed_flexible_df <- data.frame(
  "Flexible_Area_AUC" = SDMtune::auc(model = entire_easternUSA_model, test = entire_easternUSA_test),
  "Fixed_Area_AUC" = entire_fixed_ROC_AUC_value
)

# write to csv
write.csv(entire_AUC_fixed_flexible_df, file = file.path(mypath, "entire_easternUSA_fixed_flexible_AUCs.csv"), row.names = FALSE)

```

### Create distribution map for area of interest

I will use the predict function again, but this time to predict the suitability for the range of interest. The output will be a raster of suitability. This will not be done for every model in this step, but will be done for the entire models with and without ATC (to compare which is a better fit).

```{r predict function to create raster}

# output directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_step1")

# write .asc file of avg predictions
SDMtune::predict(
  object = entire_easternUSA_model, 
  data = x_env_covariates, # the covariate layers used to train the model
  fun = "mean", 
  type = "cloglog", 
  clamp = FALSE, 
  progress = TRUE, 
  filename = file.path(mypath, "entire_easternUSA_predicted_suitability.asc"), 
    # the function automatically adds the function name on the end
  filetype = "AAIGrid"
) 

```

I will convert the raster to a data frame and use this to plot it and save a jpg.

```{r convert to df and plot}

# path to directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_step1")

# load in mean predictions
entire_easternUSA_suit <- terra::rast(x = file.path(mypath, "entire_easternUSA_predicted_suitability_mean.asc")) %>%
  # terra::subst(., from = -9999, to = NA) %>% # change -9999 to NA for plotting
  terra::as.data.frame(., xy = TRUE) 
  
# plot
entire_easternUSA_suit_plot <- ggplot() +
  geom_raster(data = entire_easternUSA_suit, 
              aes(x = x, y = y, fill = mean)) +
  labs(title = "Suitability for SLF") +
  viridis::scale_fill_viridis(option = "D") +
  map_style

# save plot output
ggsave(entire_easternUSA_suit_plot, 
           filename = file.path(mypath, "plots", "entire_easternUSA_predicted_suitability_mean.jpg"),
           height = 8, 
           width = 10,
           device = "jpeg",
           dpi = "retina")

```


The next three models will follow roughly the same structure, so comments will be kept to a minimum because the workflow will be the same. 


# MaxEnt Model- entire eastern USA, without "access to cities" covariate

This model will be used in the final AUC comparison. This model will also have its own `x_env_covariates` object that does not include the ATC variable

## Tidy datasets for modeling

```{r load in files for all maxent models}

# path to directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/historical_climate_rasters/chelsa2.1_30arcsec")


# environmental and human impact covariates.
# the env covariates used to train the model
x_env_covariates_list_noATC <- list.files(path = file.path(mypath, "v1_maxent"), pattern = '\\_easternUSA.asc$', full.names = TRUE) %>%
  grep(pattern = "atc_2015", value = TRUE, invert = TRUE)

# slf presences
p_slf_points <- read.csv(file = file.path(here(), "vignette-outputs", "data-tables", "slf_all_final_coords_v1_2023-08.csv")) %>%
  dplyr::select(-species)

# entire eastern USA background point set
a_entire_easternUSA_background_points <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "easternUSA_entire_flexible_area_points.csv"))

```

```{r stack env covariates and make naming consistent}

# stack env covariates
x_env_covariates_noATC <- terra::rast(x = x_env_covariates_list_noATC)

# attributes
nlyr(x_env_covariates_noATC)
names(x_env_covariates_noATC)
minmax(x_env_covariates_noATC)

# layer name object
env_layer_names <- c("bio11_1981_2010", "bio12_1981_2010", "bio15_1981_2010", "bio2_1981_2010")



# I will change the name of the variables because they are throwing errors in SDMtune
names(x_env_covariates_noATC) <- env_layer_names
# confirmed- SDMtune doesnt like dashes in column names (it is read as a mathematical operation)

```

```{r crop presences to rasters}

# extent object for eastern USA
ext.obj <- terra::ext(-96.503906, -59.589844, 28.304381, 47.457809)

# conert to vector
p_slf_points_vect <- terra::vect(x = p_slf_points, geom = c("x", "y"), crs = "EPSG:4326") %>%
  # crop by extent area of interest
  terra::crop(., y = ext.obj) %>%
  # convert to geom, which gets coordinates of a spatVector
  terra::geom() 

# convert back to data frame
p_slf_points <- terra::as.data.frame(p_slf_points_vect) %>%
  dplyr::select(-c(geom, part, hole))

```

Finally, I will create an output directory folder to hold the model and its output data.

```{r create directory for file}

mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models")

# create directory for model
dir.create(path = file.path(mypath, "slf_easternUSA_entire_noATC_step1"))
# create plots subdir
dir.create(path = file.path(mypath, "slf_easternUSA_entire_noATC_step1", "plots"))

```

### create input data object

I need to create a dataset of presences and background points. This dataset will need to contain point-wise values for each of the predictor covariates. `SDMtune` takes an SWD (sample with data) object for this purpose, containing the presences and background points with associated covariate data to be fed into the model.

```{r prepare SWD data object}

if (FALSE) {

  entire_easternUSA_noATC_SWD <- SDMtune::prepareSWD(species = "Lycorma delicatula",
                                               env = x_env_covariates_noATC,
                                               p = p_slf_points,
                                               a = a_entire_easternUSA_background_points,
                                               verbose = TRUE # print helpful messages
                                               )
  
  entire_easternUSA_noATC_SWD@coords # coordinates
  entire_easternUSA_noATC_SWD@pa # presence / absence (background counted as absence)
  entire_easternUSA_noATC_SWD@data # extracted data from 

}

```

This model is for validation, so I will not save the output.

### Create training / test split

I will split the presences into training and testing, using 80% of the points for training and 20% for testing. I will then use `SDMtune::randomFold()` to split the training data into 5 partitions for cross-validation. This method was loosely adapted from Srivastava et.al, 2021. 

```{r split data for training and testing}

set.seed(4)

entire_easternUSA_noATC_trainTest <-  SDMtune::trainValTest(
  x = entire_easternUSA_noATC_SWD,
  test = 0.2,
  only_presence = TRUE
)

# separate off training data
entire_easternUSA_noATC_train <- entire_easternUSA_noATC_trainTest[[1]]
entire_easternUSA_noATC_test <- entire_easternUSA_noATC_trainTest[[2]]

entire_easternUSA_noATC_train@coords # coordinates
entire_easternUSA_noATC_train@pa # presence / absence (background counted as absence)
entire_easternUSA_noATC_train@data # extracted data from

```

```{r split data into k folds}

# create random folds
entire_easternUSA_noATC_trainFolds <- SDMtune::randomFolds(
  data = entire_easternUSA_noATC_train,
  k = 5, # 5 folds
  only_presence = TRUE,
  seed = 5 
)

```

## Train Maxent model

```{r train maxent model}

entire_easternUSA_noATC_model <- SDMtune::train(
  method = "Maxent",
  data = entire_easternUSA_noATC_train,
  folds = entire_easternUSA_noATC_trainFolds, # 5 folds for dataset
  fc = "lqpht", # feature classes set to ALL
  iter = 5000, # number of iterations
  progress = TRUE
)

```

## Evaluate maxent model

```{r compute summary statistics using custom function}

# output directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_noATC_step1")

# function to compute summary statistics
slfSpread::compute_MaxEnt_summary_statistics(
  model.obj = entire_easternUSA_noATC_model, 
  model.name = "entire_easternUSA_noATC_model", 
  mypath = mypath, 
  env.covar.obj = x_env_covariates_noATC, 
  train.obj = entire_easternUSA_noATC_train, 
  trainFolds.obj = entire_easternUSA_noATC_trainFolds, 
  test.obj = entire_easternUSA_noATC_test, 
  plot.types = c("train", "test"), 
  threshold.types = c("cloglog", "logistic")
  )

```

### Fixed area AUC

```{r}

slfSpread::compute_MaxEnt_fixed_ROC()

```


### Flexible vs Fixed area AUC plots

I will export the AUC values for the fixed vs flexible background points as a data frame. These will be used later to create a plot for the fixed and flexible area AUCs for all 3 models, in order to discern the best background size.

```{r Fixed and flexible AUC}

# output dorectory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_noATC_step1")

# redefine AUC object
entire_noATC_fixed_ROC_AUC_value <- as.numeric(print(entire_noATC_fixed_ROC_AUC)) %>%
  round(., digits = 7)

entire_noATC_AUC_fixed_flexible_df <- data.frame(
  "Flexible_Area_AUC" = SDMtune::auc(model = entire_easternUSA_noATC_model, test = entire_easternUSA_noATC_test),
  "Fixed_Area_AUC" = entire_noATC_fixed_ROC_AUC_value
)

# write to csv
write.csv(entire_noATC_AUC_fixed_flexible_df, file = file.path(mypath, "entire_easternUSA_noATC_fixed_flexible_AUCs.csv"), row.names = FALSE)

```

### Create distribution map for area of interest

I will use the predict function again, but this time to predict the suitability for the range of interest. The output will be a raster of suitability.

```{r predict function to create raster}

# output directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_noATC_step1")

# write .asc file of avg predictions
SDMtune::predict(
  object = entire_easternUSA_noATC_model, 
  data = x_env_covariates_noATC, # the covariate layers used to train the model
  fun = "mean", 
  type = "cloglog", 
  clamp = FALSE, 
  progress = TRUE, 
  filename = file.path(mypath, "entire_easternUSA_noATC_predicted_suitability.asc"),
  filetype = "AAIGrid"
) 

```

I will convert the raster to a data frame and use this to plot it and save a jpg.

```{r convert to df and plot}

# path to directory
mypath <- file.path(here() %>% 
                       dirname(),
                     "maxent/models/slf_easternUSA_entire_noATC_step1")

# load in mean predictions
entire_easternUSA_noATC_suit <- terra::rast(x = file.path(mypath, "entire_easternUSA_noATC_predicted_suitability_mean.asc")) %>%
  # terra::subst(., from = -9999, to = NA) %>% # change -9999 to NA for plotting
  terra::as.data.frame(., xy = TRUE) 
  
# plot
entire_easternUSA_noATC_suit_plot <- ggplot() +
  geom_raster(data = entire_easternUSA_noATC_suit, 
              aes(x = x, y = y, fill = mean)) +
  labs(title = "Suitability for SLF") +
  viridis::scale_fill_viridis(option = "D") +
  map_style

# save plot output
ggsave(entire_easternUSA_noATC_suit_plot, 
           filename = file.path(mypath, "plots", "entire_easternUSA_noATC_predicted_suitability_mean.jpg"),
           height = 8, 
           width = 10,
           device = "jpeg",
           dpi = "retina")

```


# MaxEnt Model- DD 1% adult emergence


```{r}
# DD model used with 1% adult emergence threshold
a_adult_emergence_1_background_points <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "easternUSA_adult_emergence-1_flexible_area_points.csv")) 

```






```{r compute summary statistics using custom function}

slfSpread::compute_MaxEnt_summary_statistics(
  model.obj = , 
  model.name = , 
  mypath = mypath, 
  env.covar.obj = , 
  train.obj = , 
  trainFolds.obj = , 
  test.obj = , 
  plot.types = c("train", "test"), 
  threshold.types = c("cloglog", "logistic")
  )

```




# MaxEnt Model- 355km buffer around presences

```{r create output directory for model}

# 355km buffer around slf points
a_slf_buffered_background_points <- read.csv(file.path(here(), "vignette-outputs", "data-tables", "easternUSA_slf_points_buffered_flexible_area_points.csv"))

```






```{r compute summary statistics using custom function}

slfSpread::compute_MaxEnt_summary_statistics(
  model.obj = , 
  model.name = , 
  mypath = mypath, 
  env.covar.obj = , 
  train.obj = , 
  trainFolds.obj = , 
  test.obj = , 
  plot.types = c("train", "test"), 
  threshold.types = c("cloglog", "logistic")
  )

```

# Results- Fixed and Flexible AUC comparison for all 3 models

PLOT should be a boxplot, with the background size on the x-axis (3 categories) and the fixed-area AUC value on the y-axis.
Another plot could be made using the same data, except the flexible-area AUC on the y-axis





# References

Feng, X. 2022, April 24. shandongfx/nimbios_enm. GitHub. Accessed on 2023-9-18.

Steven Phillips. (2017). A Brief Tutorial on Maxent. http://biodiversityinformatics.amnh.org/open_source/maxent/.

Steven J. Phillips, Miroslav Dudík, Robert E. Schapire. [Internet] Maxent software for modeling species niches and distributions (Version 3.4.4). Available from url: http://biodiversityinformatics.amnh.org/open_source/maxent/. Accessed on 2023-9-18.

Srivastava, V., A. D. Roe, M. A. Keena, R. C. Hamelin, and V. C. Griess. 2021. Oh the places they’ll go: improving species distribution modelling for invasive forest pests in an uncertain world. Biological Invasions 23:297–349.

VanDerWal, J., Shoo, L. P., Graham, C., & Williams, S. E. (2009). Selecting pseudo-absence data for presence-only distribution modeling: How far should you stray from what you know? Ecological Modelling, 220(4), 589–594. https://doi.org/10.1016/j.ecolmodel.2008.11.010

Vignali, S., A. Barras, V. Braunisch, and C. B.-U. of Bern. 2023, July 3. SDMtune: Species Distribution Model Selection.


